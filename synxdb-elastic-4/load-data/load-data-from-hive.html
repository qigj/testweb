

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Load Data from Hive Data Warehouse &mdash; SynxDB Elastic Documentation Preview documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=53dc447f" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=f1f14d95"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../_static/copybutton.js?v=529101c1"></script>
      <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Load Data Using gpfdist" href="load-data-using-gpfdist.html" />
    <link rel="prev" title="Load Data from Object Storage and HDFS" href="load-data-from-oss-and-hdfs.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            SynxDB Elastic Documentation
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Table of Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../product-overview/product-index.html">Product Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy-guides/deploy-guides-index.html">Deployment</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="load-data-index.html">Load Data</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="load-data-using-copy.html">Load Data Using COPY</a></li>
<li class="toctree-l2"><a class="reference internal" href="load-external-data-source-using-foreign-table.html">Load External Data Using Foreign Table</a></li>
<li class="toctree-l2"><a class="reference internal" href="load-data-using-kafka-fdw.html">Load Data from Kafka Using Kafka FDW</a></li>
<li class="toctree-l2"><a class="reference internal" href="load-data-from-oss-and-hdfs.html">Load Data from Object Storage and HDFS</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Load Data from Hive Data Warehouse</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#step-1-configure-hive-and-hdfs-information-on-product-name">Step 1. Configure Hive and HDFS information on SynxDB Elastic</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#example">Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuration-options">Configuration options</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#step-2-create-foreign-data-wrapper-and-hive-connector-extension">Step 2. Create foreign data wrapper and Hive Connector extension</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-3-create-server-and-user-mapping">Step 3. Create server and user mapping</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-4-sync-hive-objects-to-product-name">Step 4. Sync Hive objects to SynxDB Elastic</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#syncing-a-hive-table">Syncing a Hive table</a></li>
<li class="toctree-l4"><a class="reference internal" href="#more-examples">More examples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#supported-usage-and-limitations">Supported usage and limitations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#supported-hive-file-formats">Supported Hive file formats</a></li>
<li class="toctree-l4"><a class="reference internal" href="#data-type-mapping">Data type mapping</a></li>
<li class="toctree-l4"><a class="reference internal" href="#usage-limitations">Usage limitations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="load-data-using-gpfdist.html">Load Data Using <code class="docutils literal notranslate"><span class="pre">gpfdist</span></code></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../operate-with-data/operate-with-data-index.html">Operate with Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize-performance/optimize-performance-index.html">Optimize Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../manage-system/manage-system-index.html">Manage System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../security-and-permissions/set-security-and-permission-index.html">Configure Security and Permissions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../develop-guides/develop-with-db-index.html">Developer Guides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/tutorials-index.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/references-index.html">Reference Guide</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">SynxDB Elastic Documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="load-data-index.html">Load Data</a></li>
      <li class="breadcrumb-item active">Load Data from Hive Data Warehouse</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="load-data-from-hive-data-warehouse">
<h1>Load Data from Hive Data Warehouse<a class="headerlink" href="#load-data-from-hive-data-warehouse" title="Link to this heading"></a></h1>
<p>Hive data warehouse is built on the HDFS of the Hadoop cluster, so the data in the Hive data warehouse is also stored in HDFS. Currently, SynxDB Elastic supports writing data to and reading data from HDFS (see <a class="reference internal" href="load-data-from-oss-and-hdfs.html#load-data-from-object-storage-and-hdfs"><span class="std std-ref">Load Data from Object Storage and HDFS</span></a>) as well as reading data from Hive via the Hive Connector.</p>
<p>The Hive Connector loads tables from the Hive cluster as foreign tables in SynxDB Elastic, which store the paths to the data in HDFS. <code class="docutils literal notranslate"><span class="pre">datalake_fdw</span></code> reads data from these external tables, thus loading data from Hive into SynxDB Elastic.</p>
<p>The general steps to use the Hive Connector are as follows.</p>
<section id="step-1-configure-hive-and-hdfs-information-on-product-name">
<h2>Step 1. Configure Hive and HDFS information on SynxDB Elastic<a class="headerlink" href="#step-1-configure-hive-and-hdfs-information-on-product-name" title="Link to this heading"></a></h2>
<p>On containerized SynxDB Elastic, you need to create configuration files. The general steps are as follows:</p>
<ol class="arabic simple">
<li><p>Add the Hive MetaStore Service domain name and the namenode domain name to the CoreDNS server. If the two services use IP addresses, skip this step.</p></li>
<li><p>Update <code class="docutils literal notranslate"><span class="pre">gphive.conf</span></code> and <code class="docutils literal notranslate"><span class="pre">gphdfs.conf</span></code> in <code class="docutils literal notranslate"><span class="pre">connector-config</span></code> to add the Hive and HDFS information to your account namespace.</p></li>
<li><p>If the Hive cluster uses Kerberos for authentication, configures <code class="docutils literal notranslate"><span class="pre">krb5.conf</span></code> and <code class="docutils literal notranslate"><span class="pre">keytab</span></code> to add information required for Kerberos authentication.</p></li>
</ol>
<section id="example">
<h3>Example<a class="headerlink" href="#example" title="Link to this heading"></a></h3>
<ol class="arabic">
<li><p>If the Hive MetaStore Service and namenode do not use IP address, add the Hive MetaStore Service domain name and the namenode domain name to the <code class="docutils literal notranslate"><span class="pre">coredns</span></code> server in the target namespace (for example <code class="docutils literal notranslate"><span class="pre">kube-system</span></code>). You might need to rebuild the <code class="docutils literal notranslate"><span class="pre">coredns</span></code> service to apply the changes.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>kubectl<span class="w"> </span>-nkube-system<span class="w"> </span>edit<span class="w"> </span>cm<span class="w"> </span>coredns
</pre></div>
</div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ConfigMap</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">coredns</span>
<span class="nt">data</span><span class="p">:</span>
<span class="nt">Corefile</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">   </span><span class="no">.:5353 {</span>
<span class="w">      </span><span class="no">hosts {</span>
<span class="w">         </span><span class="no">10.13.9.156 10-13-9-156</span>
<span class="w">         </span><span class="no">fallthrough</span>
<span class="w">      </span><span class="no">}</span>
<span class="w">   </span><span class="no">}</span>
</pre></div>
</div>
</li>
<li><p>Configure <code class="docutils literal notranslate"><span class="pre">gphive.conf</span></code> and <code class="docutils literal notranslate"><span class="pre">gphdfs.conf</span></code> in <code class="docutils literal notranslate"><span class="pre">connector-config</span></code> in your account namespace.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>kubectl<span class="w"> </span>edit<span class="w"> </span>configmap<span class="w"> </span>connector-config<span class="w"> </span>-n<span class="w"> </span>&lt;your-account-namespace&gt;
</pre></div>
</div>
<p>The following are examples of <code class="docutils literal notranslate"><span class="pre">gphive.conf</span></code> and <code class="docutils literal notranslate"><span class="pre">gphdfs.conf</span></code> in different authentication modes.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>You need to replace the configuration options with your own ones. For the detailed description of each option, see <a class="reference internal" href="#configuration-options"><span class="std std-ref">Configuration options</span></a>.</p></li>
<li><p>In the configuration files, configuration options under cluster names must be indented with 4 spaces to align with the cluster name lines. For example, in the following example, the configuration options (such as <code class="docutils literal notranslate"><span class="pre">hdfs_namenode_host</span></code> and <code class="docutils literal notranslate"><span class="pre">hdfs_namenode_port</span></code>) under <code class="docutils literal notranslate"><span class="pre">hive-cluster-1</span></code> must be indented with 4 spaces.</p></li>
</ul>
</div>
<ul>
<li><p>For <code class="docutils literal notranslate"><span class="pre">simple</span></code> authentication mode with a single cluster.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ConfigMap</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">connector-config</span>
<span class="nt">data</span><span class="p">:</span>
<span class="nt">gphdfs.conf</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">   </span><span class="no">hdfs-cluster-1:</span>
<span class="w">       </span><span class="no"># namenode host</span>
<span class="w">       </span><span class="no">hdfs_namenode_host: 10-13-9-156</span>
<span class="w">       </span><span class="no"># name port</span>
<span class="w">       </span><span class="no">hdfs_namenode_port: 9000</span>
<span class="w">       </span><span class="no"># authentication method</span>
<span class="w">       </span><span class="no">hdfs_auth_method: simple</span>
<span class="w">       </span><span class="no"># rpc protection</span>
<span class="w">       </span><span class="no">hadoop_rpc_protection: authentication</span>
<span class="nt">gphive.conf</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">   </span><span class="no">hive-cluster-1:</span>
<span class="w">       </span><span class="no">uris: thrift://10-13-9-156:9083</span>
<span class="w">       </span><span class="no">auth_method: simple</span>
</pre></div>
</div>
</li>
<li><p>For <code class="docutils literal notranslate"><span class="pre">kerberos</span></code> authentication mode with 2 clusters for high availability.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ConfigMap</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">connector-config</span>
<span class="nt">data</span><span class="p">:</span>
<span class="nt">gphdfs.conf</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">   </span><span class="no">hdfs-cluster-1:</span>
<span class="w">       </span><span class="no">hdfs_namenode_host: mycluster</span>
<span class="w">       </span><span class="no">hdfs_namenode_port: 9000</span>
<span class="w">       </span><span class="no">hdfs_auth_method: kerberos</span>
<span class="w">       </span><span class="no">krb_principal: hdfs/10-13-9-156@EXAMPLE.COM</span>
<span class="w">       </span><span class="no">krb_principal_keytab: /etc/kerberos/keytab/hdfs.keytab</span>
<span class="w">       </span><span class="no">is_ha_supported: true</span>
<span class="w">       </span><span class="no">hadoop_rpc_protection: authentication</span>
<span class="w">       </span><span class="no">data_transfer_protocol: true</span>
<span class="w">       </span><span class="no">dfs.nameservices: mycluster</span>
<span class="w">       </span><span class="no">dfs.ha.namenodes.mycluster: nn1,nn2</span>
<span class="w">       </span><span class="no">dfs.namenode.rpc-address.mycluster.nn1: 10.13.9.156:9000</span>
<span class="w">       </span><span class="no">dfs.namenode.rpc-address.mycluster.nn2: 10.13.9.157:9000</span>
<span class="w">       </span><span class="no">dfs.client.failover.proxy.provider.mycluster: org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</span>
<span class="nt">gphive.conf</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">   </span><span class="no">hive-cluster-1:</span>
<span class="w">       </span><span class="no">uris: thrift://10.13.9.156:9083,thrift://10.13.9.157:9083</span>
<span class="w">       </span><span class="no">auth_method: kerberos</span>
<span class="w">       </span><span class="no">krb_service_principal: hive/10-13-9-156@EXAMPLE.COM</span>
<span class="w">       </span><span class="no">krb_client_principal: hive/10-13-9-156@EXAMPLE.COM</span>
<span class="w">       </span><span class="no">krb_client_keytab: /etc/kerberos/keytab/hive.keytab</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>If the target Hive cluster uses Kerberos for authentication, in addition to <code class="docutils literal notranslate"><span class="pre">gphive.conf</span></code> and <code class="docutils literal notranslate"><span class="pre">gphdfs.conf</span></code>, you also need to configure <code class="docutils literal notranslate"><span class="pre">kerberos-config</span></code> and <code class="docutils literal notranslate"><span class="pre">keytab</span></code> that exist in the proxy and all segments.</p>
<ul>
<li><p>To configure <code class="docutils literal notranslate"><span class="pre">kerberos-config</span></code>, run <code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">-n&lt;account</span> <span class="pre">namesapce&gt;</span> <span class="pre">edit</span> <span class="pre">cm</span> <span class="pre">kerberos-config</span></code>. The following is an example of <code class="docutils literal notranslate"><span class="pre">kerberos-config</span></code> to be configured. You can get the configuration information from the <code class="docutils literal notranslate"><span class="pre">krb5.conf</span></code> file of the target Hive cluster.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ConfigMap</span>
<span class="nt">data</span><span class="p">:</span>
<span class="w">   </span><span class="nt">krb5.conf</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">      </span><span class="no">[logging]</span>
<span class="w">       </span><span class="no">default = FILE:/var/log/krb5libs.log</span>
<span class="w">       </span><span class="no">kdc = FILE:/var/log/krb5kdc.log</span>
<span class="w">       </span><span class="no">admin_server = FILE:/var/log/kadmind.log</span>
<span class="w">      </span><span class="no">[libdefaults]</span>
<span class="w">       </span><span class="no">dns_lookup_realm = false</span>
<span class="w">       </span><span class="no">ticket_lifetime = 24h</span>
<span class="w">       </span><span class="no">renew_lifetime = 7d</span>
<span class="w">       </span><span class="no">forwardable = true</span>
<span class="w">       </span><span class="no">rdns = false</span>
<span class="w">       </span><span class="no">pkinit_anchors = FILE:/etc/pki/tls/certs/ca-bundle.crt</span>
<span class="w">       </span><span class="no">default_realm = EXAMPLE.COM</span>
<span class="w">      </span><span class="no">[realms]</span>
<span class="w">       </span><span class="no">EXAMPLE.COM = {</span>
<span class="w">       </span><span class="no">kdc = 10.13.9.156</span>
<span class="w">       </span><span class="no">admin_server = 10.13.9.156</span>
<span class="w">      </span><span class="no">}</span>
<span class="w">      </span><span class="no">[domain_realm]</span>
<span class="w">       </span><span class="no">.example.com = EXAMPLE.COM</span>
<span class="w">       </span><span class="no">example.com = EXAMPLE.COM</span>
</pre></div>
</div>
</li>
<li><p>To configure <code class="docutils literal notranslate"><span class="pre">keytab</span></code>, you first need to get the <code class="docutils literal notranslate"><span class="pre">hdfs.keytab</span></code> and <code class="docutils literal notranslate"><span class="pre">hive.keytab</span></code> files, and run commands in the same directory to load the files into the cluster. For example:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># Loads hdfs.keytab into the cluster.</span>
<span class="l l-Scalar l-Scalar-Plain">kubectl -n&lt;account_namespace&gt; get secret kerberos-keytab -o json | jq --arg new_value &quot;$(base64 -i hdfs.keytab)&quot; &#39;.data[&quot;hdfs.keytab&quot;] = $new_value&#39; | kubectl -n&lt;account_namespace&gt; apply -f -</span>

<span class="l l-Scalar l-Scalar-Plain"># Loads hive.keytab into the cluster.</span>
<span class="l l-Scalar l-Scalar-Plain">kubectl -n&lt;account_namespace&gt; get secret kerberos-keytab -o json | jq --arg new_value &quot;$(base64 -i hive.keytab)&quot; &#39;.data[&quot;hive.keytab&quot;] = $new_value&#39; | kubectl -n&lt;account_namespace&gt; apply -f -</span>
</pre></div>
</div>
<p>After the loading, use the following commands to check the validity of <code class="docutils literal notranslate"><span class="pre">hdfs.keytab</span></code> and <code class="docutils literal notranslate"><span class="pre">hive.keytab</span></code>. The <code class="docutils literal notranslate"><span class="pre">keytab</span></code> is stored in <code class="docutils literal notranslate"><span class="pre">/etc/kerberos/keytab/</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>kinit<span class="w"> </span>-k<span class="w"> </span>-t<span class="w"> </span>hdfs.keytab<span class="w"> </span>hdfs/10-13-9-156@EXAMPLE.COM
kinit<span class="w"> </span>-k<span class="w"> </span>-t<span class="w"> </span>hive.keytab<span class="w"> </span>hive/10-13-9-156@EXAMPLE.COM
</pre></div>
</div>
</li>
</ul>
</li>
</ol>
</section>
<section id="configuration-options">
<h3>Configuration options<a class="headerlink" href="#configuration-options" title="Link to this heading"></a></h3>
<p>This section introduces the detailed description of the configuration options of <code class="docutils literal notranslate"><span class="pre">gphive.conf</span></code> and <code class="docutils literal notranslate"><span class="pre">gphdfs.conf</span></code>.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>The default port for the <code class="docutils literal notranslate"><span class="pre">data_lake</span></code> agent has been changed from <code class="docutils literal notranslate"><span class="pre">5888</span></code> to <code class="docutils literal notranslate"><span class="pre">3888</span></code> to avoid conflict with PXF.</p>
</div>
<section id="gphive-conf">
<h4>gphive.conf<a class="headerlink" href="#gphive-conf" title="Link to this heading"></a></h4>
<p>You can get these configuration information from the <code class="docutils literal notranslate"><span class="pre">hive-site.xml</span></code> file of the target Hive cluster.</p>
<table class="docutils align-left">
<colgroup>
<col style="width: 22.2%" />
<col style="width: 55.6%" />
<col style="width: 22.2%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Item name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Default value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>uris</p></td>
<td><p>The listening address of Hive Metastore Service (the HMS hostname).</p></td>
<td><p>/</p></td>
</tr>
<tr class="row-odd"><td><p>auth_method</p></td>
<td><p>The authentication method for Hive Metastore Service: <code class="docutils literal notranslate"><span class="pre">simple</span></code> or <code class="docutils literal notranslate"><span class="pre">kerberos</span></code>.</p></td>
<td><p>simple</p></td>
</tr>
<tr class="row-even"><td><p>krb_service_principal</p></td>
<td><p>The service principal required for the Kerberos authentication of Hive Metastore Service. When using the HMS HA feature, you need to configure the instance in the principal as <code class="docutils literal notranslate"><span class="pre">_HOST</span></code>, for example, <code class="docutils literal notranslate"><span class="pre">hive/_HOST&#64;EXAMPLE</span></code>.</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>krb_client_principal</p></td>
<td><p>The client principal required for the Kerberos authentication of Hive Metastore Service.</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>krb_client_keytab</p></td>
<td><p>The keytab file of the client principal required for the Kerberos authentication of Hive Metastore Service.</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>debug</p></td>
<td><p>The debug flag of Hive Connector: true or false.</p></td>
<td><p>false</p></td>
</tr>
</tbody>
</table>
</section>
<section id="gphdfs-conf">
<h4>gphdfs.conf<a class="headerlink" href="#gphdfs-conf" title="Link to this heading"></a></h4>
<p>You can get these configuration information from the <code class="docutils literal notranslate"><span class="pre">hive-site.xml</span></code> and <code class="docutils literal notranslate"><span class="pre">hdfs-site.xml</span></code> files of the target Hive cluster.</p>
<table class="docutils align-left">
<colgroup>
<col style="width: 22.2%" />
<col style="width: 55.6%" />
<col style="width: 22.2%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Option name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Default Value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>hdfs_namenode_host</p></td>
<td><p>Configures the host information of HDFS. For example, <code class="docutils literal notranslate"><span class="pre">hdfs://mycluster</span></code>, where <code class="docutils literal notranslate"><span class="pre">hdfs://</span></code> can be omitted.</p></td>
<td><p>/</p></td>
</tr>
<tr class="row-odd"><td><p>hdfs_namenode_port</p></td>
<td><p>Configures the port information of HDFS. If not configured, the default port <code class="docutils literal notranslate"><span class="pre">9000</span></code> is used.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">9000</span></code></p></td>
</tr>
<tr class="row-even"><td><p>hdfs_auth_method</p></td>
<td><p>Configures the HDFS authentication method. Uses <code class="docutils literal notranslate"><span class="pre">simple</span></code> for regular HDFS. Uses <code class="docutils literal notranslate"><span class="pre">kerberos</span></code> for HDFS with Kerberos.</p></td>
<td><p>/</p></td>
</tr>
<tr class="row-odd"><td><p>krb_principal</p></td>
<td><p>Kerberos principal. This is set when <code class="docutils literal notranslate"><span class="pre">hdfs_auth_method</span></code> is set to “kerberos”.</p></td>
<td><p>/</p></td>
</tr>
<tr class="row-even"><td><p>krb_principal_keytab</p></td>
<td><p>The location where the user-generated keytab is placed.</p></td>
<td><p>/</p></td>
</tr>
<tr class="row-odd"><td><p>hadoop_rpc_protection</p></td>
<td><p>Should match the configuration in <code class="docutils literal notranslate"><span class="pre">hdfs-site.xml</span></code> of the HDFS cluster.</p></td>
<td><p>/</p></td>
</tr>
<tr class="row-even"><td><p>data_transfer_protocol</p></td>
<td><p>When the HDFS cluster is configured with Kerberos, there are two different methods: 1. privileged resources. 2. SASL RPC data transfer protection and SSL for HTTP. If it is the second method, “SASL”, you need to set <code class="docutils literal notranslate"><span class="pre">data_transfer_protocol</span></code> to <code class="docutils literal notranslate"><span class="pre">true</span></code> here.</p></td>
<td><p>/</p></td>
</tr>
<tr class="row-odd"><td><p>is_ha_supported</p></td>
<td><p>Sets whether to use <code class="docutils literal notranslate"><span class="pre">hdfs-ha</span></code>. The value of <code class="docutils literal notranslate"><span class="pre">true</span></code> means to use <code class="docutils literal notranslate"><span class="pre">hdfs-ha</span></code>, while <code class="docutils literal notranslate"><span class="pre">false</span></code> means not to use. The default value is <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
<section id="step-2-create-foreign-data-wrapper-and-hive-connector-extension">
<h2>Step 2. Create foreign data wrapper and Hive Connector extension<a class="headerlink" href="#step-2-create-foreign-data-wrapper-and-hive-connector-extension" title="Link to this heading"></a></h2>
<p>Before synchronization, load the <code class="docutils literal notranslate"><span class="pre">datalake_fdw</span></code> extension used for reading HDFS, and create the foreign data wrapper for reading external tables.</p>
<ol class="arabic">
<li><p>Create the necessary extensions.</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">CREATE</span><span class="w"> </span><span class="n">EXTENSION</span><span class="w"> </span><span class="n">synxdb</span><span class="p">;</span>
<span class="k">CREATE</span><span class="w"> </span><span class="n">EXTENSION</span><span class="w"> </span><span class="n">dfs_tablespace</span><span class="p">;</span>
<span class="k">CREATE</span><span class="w"> </span><span class="n">EXTENSION</span><span class="w"> </span><span class="n">gp_toolkit</span><span class="p">;</span>
<span class="k">CREATE</span><span class="w"> </span><span class="n">EXTENSION</span><span class="w"> </span><span class="n">datalake_fdw</span><span class="p">;</span>
</pre></div>
</div>
</li>
<li><p>Create the foreign data wrapper.</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">CREATE</span><span class="w"> </span><span class="k">FOREIGN</span><span class="w"> </span><span class="k">DATA</span><span class="w"> </span><span class="n">WRAPPER</span><span class="w"> </span><span class="n">datalake_fdw</span>
<span class="k">HANDLER</span><span class="w"> </span><span class="n">datalake_fdw_handler</span>
<span class="k">VALIDATOR</span><span class="w"> </span><span class="n">datalake_fdw_validator</span>
<span class="k">OPTIONS</span><span class="w"> </span><span class="p">(</span><span class="n">mpp_execute</span><span class="w"> </span><span class="s1">&#39;all segments&#39;</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p>Before calling the function, you need to load the Hive Connector extension.</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">CREATE</span><span class="w"> </span><span class="n">EXTENSION</span><span class="w"> </span><span class="n">hive_connector</span><span class="p">;</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="step-3-create-server-and-user-mapping">
<h2>Step 3. Create server and user mapping<a class="headerlink" href="#step-3-create-server-and-user-mapping" title="Link to this heading"></a></h2>
<p>After creating the foreign data wrapper and Hive Connector, you need to create the server and user mapping, as shown in the following example:</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span><span class="w"> </span><span class="k">public</span><span class="p">.</span><span class="n">create_foreign_server</span><span class="p">(</span><span class="s1">&#39;sync_server&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;gpadmin&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;datalake_fdw&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;hdfs-cluster-1&#39;</span><span class="p">);</span>
</pre></div>
</div>
<p>In the above example, the <code class="docutils literal notranslate"><span class="pre">create_foreign_server</span></code> function takes the form as follows:</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="n">create_foreign_server</span><span class="p">(</span><span class="n">serverName</span><span class="p">,</span>
<span class="w">                     </span><span class="n">userMapName</span><span class="p">,</span>
<span class="w">                     </span><span class="n">dataWrapName</span><span class="p">,</span>
<span class="w">                     </span><span class="n">hdfsClusterName</span><span class="p">);</span>
</pre></div>
</div>
<p>This function creates a server and user mapping pointing to an HDFS cluster, which can be used by the Hive Connector to create foreign tables. The <code class="docutils literal notranslate"><span class="pre">datalake_fdw</span></code> uses the server configuration to read data from the corresponding HDFS cluster when accessing external tables.</p>
<p>The parameters in the function are explained as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">serverName</span></code>: The name of the server to be created.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">userMapName</span></code>: The name of the user to be created on the server.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dataWrapName</span></code>: The name of the data wrapper used for reading HDFS data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hdfsClusterName</span></code>: The name of the HDFS cluster where the Hive cluster is located, as specified in the configuration file.</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>By default, the <code class="docutils literal notranslate"><span class="pre">datalake_fdw</span></code> accesses HDFS using the system role <code class="docutils literal notranslate"><span class="pre">gpadmin</span></code>. You can use the user option in <code class="docutils literal notranslate"><span class="pre">CREATE</span> <span class="pre">USER</span> <span class="pre">MAPPING</span></code> to control which HDFS user will be used when accessing the file system. This allows finer access control to HDFS resources.</p>
<p>Example:</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">CREATE</span><span class="w"> </span><span class="n">SERVER</span><span class="w"> </span><span class="n">foreign_server</span>
<span class="k">FOREIGN</span><span class="w"> </span><span class="k">DATA</span><span class="w"> </span><span class="n">WRAPPER</span><span class="w"> </span><span class="n">datalake_fdw</span>
<span class="k">OPTIONS</span><span class="w"> </span><span class="p">(</span>
<span class="w">   </span><span class="n">protocol</span><span class="w"> </span><span class="s1">&#39;hdfs&#39;</span><span class="p">,</span>
<span class="w">   </span><span class="n">hdfs_namenodes</span><span class="w"> </span><span class="s1">&#39;hadoop-nn&#39;</span><span class="p">,</span>
<span class="w">   </span><span class="n">hdfs_port</span><span class="w"> </span><span class="s1">&#39;9000&#39;</span><span class="p">,</span>
<span class="w">   </span><span class="n">hdfs_auth_method</span><span class="w"> </span><span class="s1">&#39;simple&#39;</span><span class="p">,</span>
<span class="w">   </span><span class="n">hadoop_rpc_protection</span><span class="w"> </span><span class="s1">&#39;authentication&#39;</span><span class="p">);</span>

<span class="k">CREATE</span><span class="w"> </span><span class="k">USER</span><span class="w"> </span><span class="n">MAPPING</span><span class="w"> </span><span class="k">FOR</span><span class="w"> </span><span class="k">current_user</span><span class="w"> </span><span class="n">SERVER</span><span class="w"> </span><span class="n">foreign_server</span>
<span class="k">OPTIONS</span><span class="w"> </span><span class="p">(</span><span class="k">user</span><span class="w"> </span><span class="s1">&#39;hdfs_reader&#39;</span><span class="p">);</span>
</pre></div>
</div>
<p>In this example, the HDFS storage will be accessed with the <code class="docutils literal notranslate"><span class="pre">hdfs_reader</span></code> user rather than the default <code class="docutils literal notranslate"><span class="pre">gpadmin</span></code>. This method is recommended for managing access permissions in multi-tenant or multi-user environments.</p>
</div>
</section>
<section id="step-4-sync-hive-objects-to-product-name">
<h2>Step 4. Sync Hive objects to SynxDB Elastic<a class="headerlink" href="#step-4-sync-hive-objects-to-product-name" title="Link to this heading"></a></h2>
<section id="syncing-a-hive-table">
<h3>Syncing a Hive table<a class="headerlink" href="#syncing-a-hive-table" title="Link to this heading"></a></h3>
<p>To sync a table from Hive to SynxDB Elastic, see the following example:</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="c1">-- Syncs Hive tables in psql.</span>

<span class="n">gpadmin</span><span class="o">=#</span><span class="w"> </span><span class="k">select</span><span class="w"> </span><span class="k">public</span><span class="p">.</span><span class="n">sync_hive_table</span><span class="p">(</span><span class="s1">&#39;hive-cluster-1&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mytestdb&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;weblogs&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;hdfs-cluster-1&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;myschema.weblogs&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;sync_server&#39;</span><span class="p">);</span>
<span class="w"> </span><span class="n">sync_hive_table</span>
<span class="c1">-----------------</span>
<span class="w"> </span><span class="n">t</span>
<span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="k">row</span><span class="p">)</span>
</pre></div>
</div>
<p>The above example uses the <code class="docutils literal notranslate"><span class="pre">sync_hive_table</span></code> function to perform the synchronization. The general form of the function is as follows:</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="n">sync_hive_table</span><span class="p">(</span><span class="n">hiveClusterName</span><span class="p">,</span>
<span class="w">               </span><span class="n">hiveDatabaseName</span><span class="p">,</span>
<span class="w">               </span><span class="n">hiveTableName</span><span class="p">,</span>
<span class="w">               </span><span class="n">hdfsClusterName</span><span class="p">,</span>
<span class="w">               </span><span class="n">destTableName</span><span class="p">,</span>
<span class="w">               </span><span class="n">serverName</span><span class="p">,</span>
<span class="w">               </span><span class="n">forceSync</span><span class="p">);</span>
</pre></div>
</div>
<p>This function syncs a table to SynxDB Elastic, with both non-forced and forced modes available. When <code class="docutils literal notranslate"><span class="pre">forceSync</span></code> is set to <code class="docutils literal notranslate"><span class="pre">true</span></code>, the sync is forced, which means that if a table with the same name already exists in SynxDB Elastic, the existing table is dropped before syncing. If the <code class="docutils literal notranslate"><span class="pre">forceSync</span></code> parameter is not provided or is set to <code class="docutils literal notranslate"><span class="pre">false</span></code>, an error will occur if a table with the same name exists.</p>
<p>The parameters are explained as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">hiveClusterName</span></code>: The name of the Hive cluster where the table to be synced is located, as specified in the configuration file.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hiveDatabaseName</span></code>: The name of the database in Hive where the table to be synced belongs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hiveTableName</span></code>: The name of the table to be synced.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hdfsClusterName</span></code>: The name of the HDFS cluster where the Hive cluster is located, as specified in the configuration file.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">destTableName</span></code>: The name of the table in SynxDB Elastic where the data will be synced.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">serverName</span></code>: The name of the server to be used when creating the foreign table with the <code class="docutils literal notranslate"><span class="pre">datalake_fdw</span></code> extension.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">forceSync</span></code>: Optional parameter. Default value is <code class="docutils literal notranslate"><span class="pre">false</span></code>. Indicates whether the sync should be forced.</p></li>
</ul>
<p><strong>Sync a partitioned Hive table using sync_hive_partition_table</strong></p>
<p>SynxDB Elastic supports synchronizing only the latest partition of a Hive table using the <code class="docutils literal notranslate"><span class="pre">sync_hive_partition_table</span></code> function. This function is used to sync a single partition specified by the highest-level partition key (for example, <code class="docutils literal notranslate"><span class="pre">prov</span></code> if the table is partitioned by <code class="docutils literal notranslate"><span class="pre">prov</span></code>, <code class="docutils literal notranslate"><span class="pre">month</span></code>, and <code class="docutils literal notranslate"><span class="pre">day</span></code>). It does not support specifying lower-level partition keys directly (such as <code class="docutils literal notranslate"><span class="pre">month</span></code> or <code class="docutils literal notranslate"><span class="pre">day</span></code>), and will return an error if you attempt to do so.</p>
<p>Function prototype:</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">CREATE</span><span class="w"> </span><span class="k">OR</span><span class="w"> </span><span class="k">REPLACE</span><span class="w"> </span><span class="k">FUNCTION</span><span class="w"> </span><span class="n">sync_hive_partition_table</span><span class="p">(</span>
<span class="w">   </span><span class="n">hiveClusterName</span><span class="w"> </span><span class="nb">text</span><span class="p">,</span>
<span class="w">   </span><span class="n">hiveDatabaseName</span><span class="w"> </span><span class="nb">text</span><span class="p">,</span>
<span class="w">   </span><span class="n">hiveTableName</span><span class="w"> </span><span class="nb">text</span><span class="p">,</span>
<span class="w">   </span><span class="n">hivePartitionValue</span><span class="w"> </span><span class="nb">text</span><span class="p">,</span>
<span class="w">   </span><span class="n">hdfsClusterName</span><span class="w"> </span><span class="nb">text</span><span class="p">,</span>
<span class="w">   </span><span class="n">destTableName</span><span class="w"> </span><span class="nb">text</span>
<span class="p">)</span><span class="w"> </span><span class="k">RETURNS</span><span class="w"> </span><span class="nb">boolean</span>
<span class="k">AS</span><span class="w"> </span><span class="s1">&#39;$libdir/hive_connector&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;sync_hive_partition_table&#39;</span>
<span class="k">LANGUAGE</span><span class="w"> </span><span class="k">C</span><span class="w"> </span><span class="k">STRICT</span><span class="w"> </span><span class="k">EXECUTE</span><span class="w"> </span><span class="k">ON</span><span class="w"> </span><span class="n">MASTER</span><span class="p">;</span>
</pre></div>
</div>
<p>The parameter <code class="docutils literal notranslate"><span class="pre">hivePartitionValue</span></code> means the value for the highest-level partition key. It must be the first key in the partition column list.</p>
<p>Example Hive table:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">CREATE</span> <span class="n">TABLE</span> <span class="n">hive_table</span> <span class="p">(</span>
   <span class="nb">id</span> <span class="nb">int</span><span class="p">,</span>
   <span class="n">name</span> <span class="n">string</span>
<span class="p">)</span>
<span class="n">PARTITIONED</span> <span class="n">BY</span> <span class="p">(</span>
   <span class="n">prov</span> <span class="nb">int</span><span class="p">,</span>
   <span class="n">month</span> <span class="nb">int</span><span class="p">,</span>
   <span class="n">day</span> <span class="nb">int</span>
<span class="p">);</span>
</pre></div>
</div>
<p>Example usage:</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span><span class="w"> </span><span class="n">sync_hive_partition_table</span><span class="p">(</span>
<span class="w">   </span><span class="s1">&#39;hive-cluster-1&#39;</span><span class="p">,</span>
<span class="w">   </span><span class="s1">&#39;mydb&#39;</span><span class="p">,</span>
<span class="w">   </span><span class="s1">&#39;hive_table&#39;</span><span class="p">,</span>
<span class="w">   </span><span class="s1">&#39;06&#39;</span><span class="p">,</span>
<span class="w">   </span><span class="s1">&#39;hdfs-cluster-1&#39;</span><span class="p">,</span>
<span class="w">   </span><span class="s1">&#39;myschema.hive_table_06&#39;</span>
<span class="p">);</span>
</pre></div>
</div>
<p>This call will sync only the partition data under <code class="docutils literal notranslate"><span class="pre">prov=06</span></code>. If you try to specify values like <code class="docutils literal notranslate"><span class="pre">month=06</span></code> or <code class="docutils literal notranslate"><span class="pre">day=15</span></code>, the function will return an error.</p>
<p>Note: This function only supports specifying the value of the first partition key. Multi-level partition value specification is currently not supported.</p>
<p>Resulting external table structure:</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">mpp_table</span><span class="w"> </span><span class="p">(</span>
<span class="w">   </span><span class="n">id</span><span class="w"> </span><span class="nb">int</span><span class="p">,</span>
<span class="w">   </span><span class="n">name</span><span class="w"> </span><span class="n">string</span><span class="p">,</span>
<span class="w">   </span><span class="n">prov</span><span class="w"> </span><span class="nb">int</span><span class="p">,</span>
<span class="w">   </span><span class="k">month</span><span class="w"> </span><span class="nb">int</span><span class="p">,</span>
<span class="w">   </span><span class="k">day</span><span class="w"> </span><span class="nb">int</span>
<span class="p">)</span>
<span class="k">LOCATION</span><span class="p">(</span><span class="s1">&#39;gphdfs://example/prov=06/ hdfs_cluster_name=paa_cluster partitonkey=month,day partitionvalue=06&#39;</span><span class="p">)</span>
<span class="n">FORMAT</span><span class="w"> </span><span class="s1">&#39;xxx&#39;</span><span class="p">;</span>
</pre></div>
</div>
</section>
<section id="more-examples">
<h3>More examples<a class="headerlink" href="#more-examples" title="Link to this heading"></a></h3>
<p><strong>Sync a Hive text table</strong></p>
<ol class="arabic">
<li><p>Create the following text table in Hive.</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="c1">-- Creates the Hive table in Beeline.</span>

<span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">weblogs</span>
<span class="p">(</span>
<span class="w">    </span><span class="n">client_ip</span><span class="w">           </span><span class="n">STRING</span><span class="p">,</span>
<span class="w">    </span><span class="n">full_request_date</span><span class="w">   </span><span class="n">STRING</span><span class="p">,</span>
<span class="w">    </span><span class="k">day</span><span class="w">                 </span><span class="n">STRING</span><span class="p">,</span>
<span class="w">    </span><span class="k">month</span><span class="w">               </span><span class="n">STRING</span><span class="p">,</span>
<span class="w">    </span><span class="n">month_num</span><span class="w">           </span><span class="nb">INT</span><span class="p">,</span>
<span class="w">    </span><span class="k">year</span><span class="w">                </span><span class="n">STRING</span><span class="p">,</span>
<span class="w">    </span><span class="n">referrer</span><span class="w">            </span><span class="n">STRING</span><span class="p">,</span>
<span class="w">    </span><span class="n">user_agent</span><span class="w">          </span><span class="n">STRING</span>
<span class="p">)</span><span class="w"> </span><span class="n">STORED</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">TEXTFILE</span><span class="p">;</span>
</pre></div>
</div>
</li>
<li><p>Sync the text table to SynxDB Elastic.</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="c1">-- Syncs the Hive table in psql.</span>

<span class="n">gpadmin</span><span class="o">=#</span><span class="w"> </span><span class="k">select</span><span class="w"> </span><span class="k">public</span><span class="p">.</span><span class="n">sync_hive_table</span><span class="p">(</span><span class="s1">&#39;hive-cluster-1&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mytestdb&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;weblogs&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;hdfs-cluster-1&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;myschema.weblogs&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;sync_server&#39;</span><span class="p">);</span>
<span class="n">sync_hive_table</span>
<span class="c1">-----------------</span>
<span class="n">t</span>
<span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="k">row</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Query the external table.</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">myschema</span><span class="p">.</span><span class="n">weblogs</span><span class="w"> </span><span class="k">LIMIT</span><span class="w"> </span><span class="mi">10</span><span class="p">;</span>
</pre></div>
</div>
</li>
</ol>
<p><strong>Sync a Hive ORC table</strong></p>
<ol class="arabic">
<li><p>Create an ORC table in Hive.</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="c1">-- Creates the Hive table in Beeline.</span>
<span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">test_all_type</span>
<span class="p">(</span>
<span class="w">    </span><span class="n">column_a</span><span class="w"> </span><span class="n">tinyint</span><span class="p">,</span>
<span class="w">    </span><span class="n">column_b</span><span class="w"> </span><span class="nb">smallint</span><span class="p">,</span>
<span class="w">    </span><span class="n">column_c</span><span class="w"> </span><span class="nb">int</span><span class="p">,</span>
<span class="w">    </span><span class="n">column_d</span><span class="w"> </span><span class="nb">bigint</span><span class="p">,</span>
<span class="w">    </span><span class="n">column_e</span><span class="w"> </span><span class="nb">float</span><span class="p">,</span>
<span class="w">    </span><span class="n">column_f</span><span class="w"> </span><span class="n">double</span><span class="p">,</span>
<span class="w">    </span><span class="n">column_g</span><span class="w"> </span><span class="n">string</span><span class="p">,</span>
<span class="w">    </span><span class="n">column_h</span><span class="w"> </span><span class="k">timestamp</span><span class="p">,</span>
<span class="w">    </span><span class="n">column_i</span><span class="w"> </span><span class="nb">date</span><span class="p">,</span>
<span class="w">    </span><span class="n">column_j</span><span class="w"> </span><span class="nb">char</span><span class="p">(</span><span class="mi">20</span><span class="p">),</span>
<span class="w">    </span><span class="n">column_k</span><span class="w"> </span><span class="nb">varchar</span><span class="p">(</span><span class="mi">20</span><span class="p">),</span>
<span class="w">    </span><span class="n">column_l</span><span class="w"> </span><span class="nb">decimal</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">)</span>
<span class="p">)</span><span class="w"> </span><span class="n">STORED</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">ORC</span><span class="p">;</span>
</pre></div>
</div>
</li>
<li><p>Sync the ORC table to SynxDB Elastic:</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="c1">-- Syncs the Hive table in psql.</span>

<span class="n">gpadmin</span><span class="o">=#</span><span class="w"> </span><span class="k">select</span><span class="w"> </span><span class="k">public</span><span class="p">.</span><span class="n">sync_hive_table</span><span class="p">(</span><span class="s1">&#39;hive-cluster-1&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mytestdb&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;test_all_type&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;hdfs-cluster-1&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;myschema.test_all_type&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;sync_server&#39;</span><span class="p">);</span>
<span class="n">sync_hive_table</span>
<span class="c1">-----------------</span>
<span class="n">t</span>
<span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="k">row</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Query the external table.</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">myschema</span><span class="p">.</span><span class="n">test_all_type</span><span class="w"> </span><span class="k">LIMIT</span><span class="w"> </span><span class="mi">10</span><span class="p">;</span>
</pre></div>
</div>
</li>
</ol>
<p><strong>Sync a Hive ORC partitioned table</strong></p>
<ol class="arabic">
<li><p>Create an ORC partitioned table in Hive.</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="c1">-- Creates the Hive table in Beeline.</span>

<span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">test_partition_1_int</span>
<span class="p">(</span>
<span class="w">    </span><span class="n">a</span><span class="w"> </span><span class="n">tinyint</span><span class="p">,</span>
<span class="w">    </span><span class="n">b</span><span class="w"> </span><span class="nb">smallint</span><span class="p">,</span>
<span class="w">    </span><span class="k">c</span><span class="w"> </span><span class="nb">int</span><span class="p">,</span>
<span class="w">    </span><span class="n">d</span><span class="w"> </span><span class="nb">bigint</span><span class="p">,</span>
<span class="w">    </span><span class="n">e</span><span class="w"> </span><span class="nb">float</span><span class="p">,</span>
<span class="w">    </span><span class="n">f</span><span class="w"> </span><span class="n">double</span><span class="p">,</span>
<span class="w">    </span><span class="k">g</span><span class="w"> </span><span class="n">string</span><span class="p">,</span>
<span class="w">    </span><span class="n">h</span><span class="w"> </span><span class="k">timestamp</span><span class="p">,</span>
<span class="w">    </span><span class="n">i</span><span class="w"> </span><span class="nb">date</span><span class="p">,</span>
<span class="w">    </span><span class="n">j</span><span class="w"> </span><span class="nb">char</span><span class="p">(</span><span class="mi">20</span><span class="p">),</span>
<span class="w">    </span><span class="n">k</span><span class="w"> </span><span class="nb">varchar</span><span class="p">(</span><span class="mi">20</span><span class="p">),</span>
<span class="w">    </span><span class="n">l</span><span class="w"> </span><span class="nb">decimal</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">PARTITIONED</span><span class="w"> </span><span class="k">BY</span>
<span class="p">(</span>
<span class="w">    </span><span class="n">m</span><span class="w"> </span><span class="nb">int</span>
<span class="p">)</span>
<span class="n">STORED</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">ORC</span><span class="p">;</span>
<span class="k">INSERT</span><span class="w"> </span><span class="k">INTO</span><span class="w"> </span><span class="n">test_partition_1_int</span><span class="w"> </span><span class="k">VALUES</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;1&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;2020-01-01 01:01:01&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;2020-01-01&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;1&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;1&#39;</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">.</span><span class="mi">01</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="k">INSERT</span><span class="w"> </span><span class="k">INTO</span><span class="w"> </span><span class="n">test_partition_1_int</span><span class="w"> </span><span class="k">VALUES</span><span class="w"> </span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;2&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;2020-02-02 02:02:02&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;2020-02-01&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;2&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;2&#39;</span><span class="p">,</span><span class="w"> </span><span class="mi">11</span><span class="p">.</span><span class="mi">01</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span>
<span class="k">INSERT</span><span class="w"> </span><span class="k">INTO</span><span class="w"> </span><span class="n">test_partition_1_int</span><span class="w"> </span><span class="k">VALUES</span><span class="w"> </span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;3&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;2020-03-03 03:03:03&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;2020-03-01&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;3&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;3&#39;</span><span class="p">,</span><span class="w"> </span><span class="mi">12</span><span class="p">.</span><span class="mi">01</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">);</span>
<span class="k">INSERT</span><span class="w"> </span><span class="k">INTO</span><span class="w"> </span><span class="n">test_partition_1_int</span><span class="w"> </span><span class="k">VALUES</span><span class="w"> </span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;4&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;2020-04-04 04:04:04&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;2020-04-01&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;4&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;4&#39;</span><span class="p">,</span><span class="w"> </span><span class="mi">13</span><span class="p">.</span><span class="mi">01</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">);</span>
<span class="k">INSERT</span><span class="w"> </span><span class="k">INTO</span><span class="w"> </span><span class="n">test_partition_1_int</span><span class="w"> </span><span class="k">VALUES</span><span class="w"> </span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;5&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;2020-05-05 05:05:05&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;2020-05-01&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;5&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;5&#39;</span><span class="p">,</span><span class="w"> </span><span class="mi">14</span><span class="p">.</span><span class="mi">01</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p>Sync the ORC partitioned table to SynxDB Elastic.</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="c1">-- psql syncs the Hive partitioned tables as one foreign table.</span>

<span class="n">gpadmin</span><span class="o">=#</span><span class="w"> </span><span class="k">select</span><span class="w"> </span><span class="k">public</span><span class="p">.</span><span class="n">sync_hive_table</span><span class="p">(</span><span class="s1">&#39;hive-cluster-1&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mytestdb&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;test_partition_1_int&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;hdfs-cluster-1&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;myschema.test_partition_1_int&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;sync_server&#39;</span><span class="p">);</span>
<span class="n">sync_hive_table</span>
<span class="c1">-----------------</span>
<span class="n">t</span>
<span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="k">row</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Query the external table.</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">myschema</span><span class="p">.</span><span class="n">test_partition_1_int</span><span class="w"> </span><span class="k">LIMIT</span><span class="w"> </span><span class="mi">10</span><span class="p">;</span>
</pre></div>
</div>
</li>
</ol>
</section>
</section>
<section id="supported-usage-and-limitations">
<h2>Supported usage and limitations<a class="headerlink" href="#supported-usage-and-limitations" title="Link to this heading"></a></h2>
<section id="supported-hive-file-formats">
<h3>Supported Hive file formats<a class="headerlink" href="#supported-hive-file-formats" title="Link to this heading"></a></h3>
<p>You can load files in TEXT, CSV, ORC, or PARQUET formats from Hive into SynxDB Elastic.</p>
</section>
<section id="data-type-mapping">
<h3>Data type mapping<a class="headerlink" href="#data-type-mapping" title="Link to this heading"></a></h3>
<p>The following table shows the one-to-one mapping between table data types on a Hive cluster and table data types in SynxDB Elastic.</p>
<table class="docutils align-left">
<thead>
<tr class="row-odd"><th class="head"><p>Hive</p></th>
<th class="head"><p>SynxDB Elastic</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>binary</p></td>
<td><p>bytea</p></td>
</tr>
<tr class="row-odd"><td><p>tinyint</p></td>
<td><p>smallint</p></td>
</tr>
<tr class="row-even"><td><p>smallint</p></td>
<td><p>smallint</p></td>
</tr>
<tr class="row-odd"><td><p>int</p></td>
<td><p>int</p></td>
</tr>
<tr class="row-even"><td><p>bigint</p></td>
<td><p>bigint</p></td>
</tr>
<tr class="row-odd"><td><p>float</p></td>
<td><p>float4</p></td>
</tr>
<tr class="row-even"><td><p>double</p></td>
<td><p>double precision</p></td>
</tr>
<tr class="row-odd"><td><p>string</p></td>
<td><p>text</p></td>
</tr>
<tr class="row-even"><td><p>timestamp</p></td>
<td><p>timestamp</p></td>
</tr>
<tr class="row-odd"><td><p>date</p></td>
<td><p>date</p></td>
</tr>
<tr class="row-even"><td><p>char</p></td>
<td><p>char</p></td>
</tr>
<tr class="row-odd"><td><p>varchar</p></td>
<td><p>varchar</p></td>
</tr>
<tr class="row-even"><td><p>decimal</p></td>
<td><p>decimal</p></td>
</tr>
</tbody>
</table>
</section>
<section id="usage-limitations">
<h3>Usage limitations<a class="headerlink" href="#usage-limitations" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Synchronizing Hive external tables is not supported.</p></li>
<li><p>Synchronizing Hive table statistics is not supported.</p></li>
<li><p>SynxDB Elastic can read data from HDFS and write data to HDFS, but the written data cannot be read by Hive.</p></li>
<li><p>When using <code class="docutils literal notranslate"><span class="pre">sync_hive_partition_table</span></code>, only the first-level partition key is supported. Specifying a value from a secondary or lower-level partition key will result in an error.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Q: How is write and update on HDFS synchronized to SynxDB Elastic? Are there any limitations?</p>
<p>A: The data is still stored in HDFS, and the Foreign Data Wrapper only reads the data from HDFS.</p>
</div>
</section>
</section>
</section>


           </div>
          </div>
          


  <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      <a href="load-data-using-gpfdist.html" class="btn btn-neutral float-right" title="Load Data Using gpfdist" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      <a href="load-data-from-oss-and-hdfs.html" class="btn btn-neutral float-left" title="Load Data from Object Storage and HDFS" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
  </div>

<footer class="footer">
  <div class="container">
    <div class="footer-content">
      <div class="footer-section">
        <p>&copy; 2025, Synx Data Labs, Inc. All rights reserved.</p>
      </div>
      <div class="footer-section">
        <p>SynxDB Elastic Documentation</p>
      </div>
    </div>
  </div>
</footer>

<style>
.footer {
  background-color: var(--bg-secondary);
  border-top: 1px solid var(--border);
  padding: 1.5rem 0;
  margin-top: 3rem;
}

.footer-content {
  display: flex;
  justify-content: space-between;
  align-items: center;
  max-width: 1200px;
  margin: 0 auto;
  padding: 0 2rem;
}

.footer-section p {
  margin: 0;
  color: var(--text-secondary);
  font-size: 0.9rem;
  font-family: 'Inter', sans-serif;
}

@media (max-width: 768px) {
  .footer-content {
    flex-direction: column;
    gap: 0.5rem;
    text-align: center;
  }
}
</style>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>
<!DOCTYPE HTML>
<html lang="en" class="ayu sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>About the SynxDB Architecture - SynxDB 2 Documentation</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../../favicon.svg">
        <link rel="shortcut icon" href="../../favicon.png">
        <link rel="stylesheet" href="../../css/variables.css">
        <link rel="stylesheet" href="../../css/general.css">
        <link rel="stylesheet" href="../../css/chrome.css">
        <link rel="stylesheet" href="../../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../../highlight.css">
        <link rel="stylesheet" href="../../tomorrow-night.css">
        <link rel="stylesheet" href="../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "ayu" : "ayu";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../../toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('ayu')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">SynxDB 2 Documentation</h1>

                    <div class="right-buttons">
                        <a href="../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                        &nbsp;&nbsp;&nbsp;&nbsp;
                        <a href="https://www.synxdata.com/"><img id="fa fa-print" src="../../SYNX-Text-and-Circular-Logo-142x28-White-text-Black-background.png" alt="Synx Data Labs Logo"/></a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="about-the-synxdb-architecture"><a class="header" href="#about-the-synxdb-architecture">About the SynxDB Architecture</a></h1>
<p>SynxDB is a massively parallel processing (MPP) database server with an architecture specially designed to manage large-scale analytic data warehouses and business intelligence workloads.</p>
<p>MPP (also known as a <em>shared nothing</em> architecture) refers to systems with two or more processors that cooperate to carry out an operation, each processor with its own memory, operating system and disks. SynxDB uses this high-performance system architecture to distribute the load of multi-terabyte data warehouses, and can use all of a system’s resources in parallel to process a query.</p>
<p>SynxDB is based on PostgreSQL open-source technology. It is essentially several PostgreSQL disk-oriented database instances acting together as one cohesive database management system (DBMS). It is based on PostgreSQL 9.4, and in most cases is very similar to PostgreSQL with regard to SQL support, features, configuration options, and end-user functionality. Database users interact with SynxDB as they would with a regular PostgreSQL DBMS.</p>
<p>SynxDB can use the append-optimized (AO) storage format for bulk loading and reading of data, and provides performance advantages over HEAP tables. Append-optimized storage provides checksums for data protection, compression and row/column orientation. Both row-oriented or column-oriented append-optimized tables can be compressed.</p>
<p>The main differences between SynxDB and PostgreSQL are as follows:</p>
<ul>
<li>GPORCA is leveraged for query planning, in addition to the Postgres Planner.</li>
<li>SynxDB can use append-optimized storage.</li>
<li>SynxDB has the option to use column storage, data that is logically organized as a table, using rows and columns that are physically stored in a column-oriented format, rather than as rows. Column storage can only be used with append-optimized tables. Column storage is compressible. It also can provide performance improvements as you only need to return the columns of interest to you. All compression algorithms can be used with either row or column-oriented tables, but Run-Length Encoded (RLE) compression can only be used with column-oriented tables. SynxDB provides compression on all Append-Optimized tables that use column storage.</li>
</ul>
<p>The internals of PostgreSQL have been modified or supplemented to support the parallel structure of SynxDB. For example, the system catalog, optimizer, query executor, and transaction manager components have been modified and enhanced to be able to run queries simultaneously across all of the parallel PostgreSQL database instances. The SynxDB <em>interconnect</em> (the networking layer) enables communication between the distinct PostgreSQL instances and allows the system to behave as one logical database.</p>
<p>SynxDB also can use declarative partitions and sub-partitions to implicitly generate partition constraints.</p>
<p>SynxDB also includes features designed to optimize PostgreSQL for business intelligence (BI) workloads. For example, SynxDB has added parallel data loading (external tables), resource management, query optimizations, and storage enhancements, which are not found in standard PostgreSQL. Many features and optimizations developed by SynxDB make their way into the PostgreSQL community. For example, table partitioning is a feature first developed by SynxDB, and it is now in standard PostgreSQL.</p>
<p>SynxDB queries use a Volcano-style query engine model, where the execution engine takes an execution plan and uses it to generate a tree of physical operators, evaluates tables through physical operators, and delivers results in a query response.</p>
<p>SynxDB stores and processes large amounts of data by distributing the data and processing workload across several servers or <em>hosts</em>. SynxDB is an <em>array</em> of individual databases based upon PostgreSQL 9.4 working together to present a single database image. The <em>master</em> is the entry point to the SynxDB system. It is the database instance to which clients connect and submit SQL statements. The master coordinates its work with the other database instances in the system, called <em>segments</em>, which store and process the data.</p>
<p><img src="../graphics/highlevel_arch.jpg" alt="High-Level SynxDB Architecture" title="High-Level SynxDB Architecture" /></p>
<p>The following topics describe the components that make up a SynxDB system and how they work together.</p>
<ul>
<li><a href="#arch_master">About the SynxDB Master</a></li>
<li><a href="#arch_segments">About the SynxDB Segments</a></li>
<li><a href="#arch_interconnect">About the SynxDB Interconnect</a></li>
<li><a href="#topic13">About ETL Hosts for Data Loading</a></li>
</ul>
<h2 id="about-the-synxdb-master"><a class="header" href="#about-the-synxdb-master"><a id="arch_master"></a>About the SynxDB Master</a></h2>
<p>The SynxDB master is the entry to the SynxDB system, accepting client connections and SQL queries, and distributing work to the segment instances.</p>
<p>SynxDB end-users interact with SynxDB (through the master) as they would with a typical PostgreSQL database. They connect to the database using client programs such as <code>psql</code> or application programming interfaces (APIs) such as JDBC, ODBC or <a href="https://www.postgresql.org/docs/9.4/libpq.html">libpq</a> (the PostgreSQL C API).</p>
<p>The master is where the <em>global system catalog</em> resides. The global system catalog is the set of system tables that contain metadata about the SynxDB system itself. The master does not contain any user data; data resides only on the <em>segments</em>. The master authenticates client connections, processes incoming SQL commands, distributes workloads among segments, coordinates the results returned by each segment, and presents the final results to the client program.</p>
<p>SynxDB uses Write-Ahead Logging (WAL) for master/standby master mirroring. In WAL-based logging, all modifications are written to the log before being applied, to ensure data integrity for any in-process operations.</p>
<h3 id="master-redundancy"><a class="header" href="#master-redundancy"><a id="topic3"></a>Master Redundancy</a></h3>
<p>You may optionally deploy a <em>backup</em> or <em>mirror</em> of the master instance. A backup master host serves as a <em>warm standby</em> if the primary master host becomes nonoperational. You can deploy the standby master on a designated redundant master host or on one of the segment hosts.</p>
<p>The standby master is kept up to date by a transaction log replication process, which runs on the standby master host and synchronizes the data between the primary and standby master hosts. If the primary master fails, the log replication process shuts down, and an administrator can activate the standby master in its place. When the standby master is active, the replicated logs are used to reconstruct the state of the master host at the time of the last successfully committed transaction.</p>
<p>Since the master does not contain any user data, only the system catalog tables need to be synchronized between the primary and backup copies. When these tables are updated, changes automatically copy over to the standby master so it is always synchronized with the primary.</p>
<p><img src="../graphics/standby_master.jpg" alt="Master Mirroring in SynxDB" title="Master Mirroring in SynxDB" /></p>
<h2 id="about-the-synxdb-segments"><a class="header" href="#about-the-synxdb-segments"><a id="arch_segments"></a>About the SynxDB Segments</a></h2>
<p>SynxDB segment instances are independent PostgreSQL databases that each store a portion of the data and perform the majority of query processing.</p>
<p>When a user connects to the database via the SynxDB master and issues a query, processes are created in each segment database to handle the work of that query. For more information about query processes, see <a href="../query/topics/parallel-proc.html">About SynxDB Query Processing</a>.</p>
<p>User-defined tables and their indexes are distributed across the available segments in a SynxDB system; each segment contains a distinct portion of data. The database server processes that serve segment data run under the corresponding segment instances. Users interact with segments in a SynxDB system through the master.</p>
<p>A server that runs a segment instance is called a <em>segment host</em>. A segment host typically runs from two to eight SynxDB segments, depending on the CPU cores, RAM, storage, network interfaces, and workloads. Segment hosts are expected to be identically configured. The key to obtaining the best performance from SynxDB is to distribute data and workloads <em>evenly</em> across a large number of equally capable segments so that all segments begin working on a task simultaneously and complete their work at the same time.</p>
<h3 id="segment-redundancy"><a class="header" href="#segment-redundancy"><a id="topic5"></a>Segment Redundancy</a></h3>
<p>When you deploy your SynxDB system, you have the option to configure <em>mirror</em> segments. Mirror segments allow database queries to fail over to a backup segment if the primary segment becomes unavailable. Mirroring is a requirement for production SynxDB systems.</p>
<p>A mirror segment must always reside on a different host than its primary segment. Mirror segments can be arranged across the hosts in the system in one of two standard configurations, or in a custom configuration you design. The default configuration, called <em>group</em> mirroring, places the mirror segments for all primary segments on one other host. Another option, called <em>spread</em> mirroring, spreads mirrors for each host’s primary segments over the remaining hosts. Spread mirroring requires that there be more hosts in the system than there are primary segments on the host. On hosts with multiple network interfaces, the primary and mirror segments are distributed equally among the interfaces. This figure shows how table data is distributed across the segments when the default group mirroring option is configured:</p>
<p><img src="../graphics/group-mirroring.png" alt="Data Mirroring in SynxDB" title="Data Mirroring in SynxDB" /></p>
<h4 id="segment-failover-and-recovery"><a class="header" href="#segment-failover-and-recovery"><a id="topic6"></a>Segment Failover and Recovery</a></h4>
<p>When mirroring is enabled in a SynxDB system, the system automatically fails over to the mirror copy if a primary copy becomes unavailable. A SynxDB system can remain operational if a segment instance or host goes down only if all portions of data are available on the remaining active segments.</p>
<p>If the master cannot connect to a segment instance, it marks that segment instance as <em>invalid</em> in the SynxDB system catalog. The segment instance remains invalid and out of operation until an administrator brings that segment back online. An administrator can recover a failed segment while the system is up and running. The recovery process copies over only the changes that were missed while the segment was nonoperational.</p>
<p>If you do not have mirroring enabled and a segment becomes invalid, the system automatically shuts down. An administrator must recover all failed segments before operations can continue.</p>
<h3 id="example-segment-host-hardware-stack"><a class="header" href="#example-segment-host-hardware-stack"><a id="topic7"></a>Example Segment Host Hardware Stack</a></h3>
<p>Regardless of the hardware platform you choose, a production SynxDB processing node (a segment host) is typically configured as described in this section.</p>
<p>The segment hosts do the majority of database processing, so the segment host servers are configured in order to achieve the best performance possible from your SynxDB system. SynxDB’s performance will be as fast as the slowest segment server in the array. Therefore, it is important to ensure that the underlying hardware and operating systems that are running SynxDB are all running at their optimal performance level. It is also advised that all segment hosts in a SynxDB array have identical hardware resources and configurations.</p>
<p>Segment hosts should also be dedicated to SynxDB operations only. To get the best query performance, you do not want SynxDB competing with other applications for machine or network resources.</p>
<p>The following diagram shows an example SynxDB segment host hardware stack. The number of effective CPUs on a host is the basis for determining how many primary SynxDB segment instances to deploy per segment host. This example shows a host with two effective CPUs (one dual-core CPU). Note that there is one primary segment instance (or primary/mirror pair if using mirroring) per CPU core.</p>
<p><img src="../../install_guide/graphics/hardware_stack.jpg" alt="Example SynxDB Segment Host Configuration" title="Example SynxDB Segment Host Configuration" /></p>
<h3 id="example-segment-disk-layout"><a class="header" href="#example-segment-disk-layout"><a id="topic8"></a>Example Segment Disk Layout</a></h3>
<p>Each CPU is typically mapped to a logical disk. A logical disk consists of one primary file system (and optionally a mirror file system) accessing a pool of physical disks through an I/O channel or disk controller. The logical disk and file system are provided by the operating system. Most operating systems provide the ability for a logical disk drive to use groups of physical disks arranged in RAID arrays.</p>
<p><img src="../../install_guide/graphics/disk_raid.jpg" alt="Logical Disk Layout in SynxDB" title="Logical Disk Layout in SynxDB" /></p>
<p>Depending on the hardware platform you choose, different RAID configurations offer different performance and capacity levels. SynxDB supports and certifies a number of reference hardware platforms and operating systems. Check with your sales account representative for the recommended configuration on your chosen platform.</p>
<h2 id="about-the-synxdb-interconnect"><a class="header" href="#about-the-synxdb-interconnect"><a id="arch_interconnect"></a>About the SynxDB Interconnect</a></h2>
<p>The interconnect is the networking layer of the SynxDB architecture.</p>
<p>The <em>interconnect</em> refers to the inter-process communication between segments and the network infrastructure on which this communication relies. The SynxDB interconnect uses a standard Ethernet switching fabric. For performance reasons, a 10-Gigabit system, or faster, is recommended.</p>
<p>By default, the interconnect uses User Datagram Protocol with flow control (UDPIFC) for interconnect traffic to send messages over the network. The SynxDB software performs packet verification beyond what is provided by UDP. This means the reliability is equivalent to Transmission Control Protocol (TCP), and the performance and scalability exceeds TCP. If the interconnect is changed to TCP, SynxDB has a scalability limit of 1000 segment instances. With UDPIFC as the default protocol for the interconnect, this limit is not applicable.</p>
<h3 id="interconnect-redundancy"><a class="header" href="#interconnect-redundancy"><a id="topic10"></a>Interconnect Redundancy</a></h3>
<p>A highly available interconnect can be achieved by deploying dual 10 Gigabit Ethernet switches on your network, and redundant 10 Gigabit connections to the SynxDB master and segment host servers.</p>
<h3 id="network-interface-configuration"><a class="header" href="#network-interface-configuration"><a id="topic11"></a>Network Interface Configuration</a></h3>
<p>A segment host typically has multiple network interfaces designated to SynxDB interconnect traffic. The master host typically has additional external network interfaces in addition to the interfaces used for interconnect traffic.</p>
<p>Depending on the number of interfaces available, you will want to distribute interconnect network traffic across the number of available interfaces. This is done by assigning segment instances to a particular network interface and ensuring that the primary segments are evenly balanced over the number of available interfaces.</p>
<p>This is done by creating separate host address names for each network interface. For example, if a host has four network interfaces, then it would have four corresponding host addresses, each of which maps to one or more primary segment instances. The <code>/etc/hosts</code> file should be configured to contain not only the host name of each machine, but also all interface host addresses for all of the SynxDB hosts (master, standby master, segments, and ETL hosts).</p>
<p>With this configuration, the operating system automatically selects the best path to the destination. SynxDB automatically balances the network destinations to maximize parallelism.</p>
<p><img src="../../install_guide/graphics/multi_nic_arch.jpg" alt="Example Network Interface Architecture" title="Example Network Interface Architecture" /></p>
<h3 id="switch-configuration"><a class="header" href="#switch-configuration"><a id="topic12"></a>Switch Configuration</a></h3>
<p>When using multiple 10 Gigabit Ethernet switches within your SynxDB array, evenly divide the number of subnets between each switch. In this example configuration, if we had two switches, NICs 1 and 2 on each host would use switch 1 and NICs 3 and 4 on each host would use switch 2. For the master host, the host name bound to NIC 1 (and therefore using switch 1) is the effective master host name for the array. Therefore, if deploying a warm standby master for redundancy purposes, the standby master should map to a NIC that uses a different switch than the primary master.</p>
<p><img src="../../install_guide/graphics/multi_switch_arch.jpg" alt="Example Switch Configuration" title="Example Switch Configuration" /></p>
<h2 id="about-etl-hosts-for-data-loading"><a class="header" href="#about-etl-hosts-for-data-loading"><a id="topic13"></a>About ETL Hosts for Data Loading</a></h2>
<p>SynxDB supports fast, parallel data loading with its external tables feature. By using external tables in conjunction with SynxDB’s parallel file server (<code>gpfdist</code>), administrators can achieve maximum parallelism and load bandwidth from their SynxDB system. Many production systems deploy designated ETL servers for data loading purposes. These machines run the SynxDB parallel file server (<code>gpfdist</code>), but not SynxDB instances.</p>
<p>One advantage of using the <code>gpfdist</code> file server program is that it ensures that all of the segments in your SynxDB system are fully utilized when reading from external table data files.</p>
<p>The <code>gpfdist</code> program can serve data to the segment instances at an average rate of about 350 MB/s for delimited text formatted files and 200 MB/s for CSV formatted files. Therefore, you should consider the following options when running <code>gpfdist</code> in order to maximize the network bandwidth of your ETL systems:</p>
<ul>
<li>If your ETL machine is configured with multiple network interface cards (NICs) as described in <a href="#topic11">Network Interface Configuration</a>, run one instance of <code>gpfdist</code> on your ETL host and then define your external table definition so that the host name of each NIC is declared in the <code>LOCATION</code> clause (see <code>CREATE EXTERNAL TABLE</code> in the <em>SynxDB Reference Guide</em>). This allows network traffic between your SynxDB segment hosts and your ETL host to use all NICs simultaneously.</li>
</ul>
<p><img src="../graphics/ext_tables_multinic.jpg" alt="External Table Using Single gpfdist Instance with Multiple NICs" title="External Table Using Single gpfdist Instance with Multiple NICs" /></p>
<ul>
<li>Run multiple <code>gpfdist</code> instances on your ETL host and divide your external data files equally between each instance. For example, if you have an ETL system with two network interface cards (NICs), then you could run two <code>gpfdist</code> instances on that machine to maximize your load performance. You would then divide the external table data files evenly between the two <code>gpfdist</code> programs.</li>
</ul>
<p><img src="../graphics/ext_tables.jpg" alt="External Tables Using Multiple gpfdist Instances with Multiple NICs" title="External Tables Using Multiple gpfdist Instances with Multiple NICs" /></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../../admin_guide/intro/partI.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../../admin_guide/intro/about_utilities.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../../admin_guide/intro/partI.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../../admin_guide/intro/about_utilities.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../../elasticlunr.min.js"></script>
        <script src="../../mark.min.js"></script>
        <script src="../../searcher.js"></script>

        <script src="../../clipboard.min.js"></script>
        <script src="../../highlight.js"></script>
        <script src="../../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>

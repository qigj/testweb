<!DOCTYPE HTML>
<html lang="en" class="ayu sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Reading Hive Table Data - SynxDB 2 Documentation</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "ayu" : "ayu";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('ayu')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">SynxDB 2 Documentation</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                        &nbsp;&nbsp;&nbsp;&nbsp;
                        <a href="https://www.synxdata.com/"><img id="fa fa-print" src="../SYNX-Text-and-Circular-Logo-142x28-White-text-Black-background.png" alt="Synx Data Labs Logo"/></a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="reading-hive-table-data"><a class="header" href="#reading-hive-table-data">Reading Hive Table Data</a></h1>
<!--
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->
<p>Apache Hive is a distributed data warehousing infrastructure. Hive facilitates managing large data sets supporting multiple data formats, including comma-separated value (.csv) TextFile, RCFile, ORC, and Parquet.</p>
<p>The PXF Hive connector reads data stored in a Hive table. This section describes how to use the PXF Hive connector.</p>
<div class="note"><b>Note:</b> When accessing Hive 3, the PXF Hive connector supports using the <code>hive[:*]</code> profiles described below to access Hive 3 external tables only. The Connector does not support using the <code>hive[:*]</code> profiles to access Hive 3 managed (CRUD and insert-only transactional, and temporary) tables. Use the <a href="jdbc_pxf.html">PXF JDBC Connector</a> to access Hive 3 managed tables instead.</div>
<h2 id="prerequisites"><a class="header" href="#prerequisites"><a id="prereq"></a>Prerequisites</a></h2>
<p>Before working with Hive table data using PXF, ensure that you have met the PXF Hadoop <a href="access_hdfs.html#hadoop_prereq">Prerequisites</a>.</p>
<p><em>If you plan to use PXF filter pushdown with Hive integral types</em>, ensure that the configuration parameter <code>hive.metastore.integral.jdo.pushdown</code> exists and is set to <code>true</code> in the <code>hive-site.xml</code> file in both your Hadoop cluster <strong>and</strong> <code>$PXF_BASE/servers/default/hive-site.xml</code>. Refer to <a href="client_instcfg.html#client-cfg-update">About Updating Hadoop Configuration</a> for more information.</p>
<h2 id="hive-data-formats"><a class="header" href="#hive-data-formats"><a id="hive_fileformats"></a>Hive Data Formats</a></h2>
<p>The PXF Hive connector supports several data formats, and has defined the following profiles for accessing these formats:</p>
<div class="table-wrapper"><table><thead><tr><th>File Format</th><th>Description</th><th>Profile</th></tr></thead><tbody>
<tr><td>TextFile</td><td>Flat file with data in comma-, tab-, or space-separated value format or JSON notation.</td><td>hive, hive:text</td></tr>
<tr><td>SequenceFile</td><td>Flat file consisting of binary key/value pairs.</td><td>hive</td></tr>
<tr><td>RCFile</td><td>Record columnar data consisting of binary key/value pairs; high row compression rate.</td><td>hive, hive:rc</td></tr>
<tr><td>ORC</td><td>Optimized row columnar data with stripe, footer, and postscript sections; reduces data size.</td><td>hive, hive:orc</td></tr>
<tr><td>Parquet</td><td>Compressed columnar data representation.</td><td>hive</td></tr>
<tr><td>Avro</td><td>Serialization system with a binary data format.</td><td>hive</td></tr>
</tbody></table>
</div>
<p><strong>Note</strong>: The <code>hive</code> profile supports all file storage formats. It will use the optimal <code>hive[:*]</code> profile for the underlying file format type.</p>
<h2 id="data-type-mapping"><a class="header" href="#data-type-mapping"><a id="hive_datatypemap"></a>Data Type Mapping</a></h2>
<p>The PXF Hive connector supports primitive and complex data types.</p>
<h3 id="primitive-data-types"><a class="header" href="#primitive-data-types"><a id="hive_datatypemap_prim" class="no-quick-link"></a>Primitive Data Types</a></h3>
<p>To represent Hive data in SynxDB, map data values that use a primitive data type to SynxDB columns of the same type.</p>
<p>The following table summarizes external mapping rules for Hive primitive types.</p>
<div class="table-wrapper"><table><thead><tr><th>Hive Data Type</th><th>SynxDB Data Type</th></tr></thead><tbody>
<tr><td>boolean</td><td>bool</td></tr>
<tr><td>int</td><td>int4</td></tr>
<tr><td>smallint</td><td>int2</td></tr>
<tr><td>tinyint</td><td>int2</td></tr>
<tr><td>bigint</td><td>int8</td></tr>
<tr><td>float</td><td>float4</td></tr>
<tr><td>double</td><td>float8</td></tr>
<tr><td>string</td><td>text</td></tr>
<tr><td>binary</td><td>bytea</td></tr>
<tr><td>timestamp</td><td>timestamp</td></tr>
</tbody></table>
</div>
<p><strong>Note</strong>: The <code>hive:orc</code> profile does not support the timestamp data type when you specify vectorized query execution (<code>VECTORIZE=true</code>).</p>
<h3 id="complex-data-types"><a class="header" href="#complex-data-types"><a id="hive_datatypemap_complex" class="no-quick-link"></a>Complex Data Types</a></h3>
<p>Hive supports complex data types including array, struct, map, and union. PXF maps each of these complex types to <code>text</code>. You can create SynxDB functions or application code to extract subcomponents of these complex data types.</p>
<p>Examples using complex data types with the <code>hive</code> and <code>hive:orc</code> profiles are provided later in this topic.</p>
<p><strong>Note</strong>: The <code>hive:orc</code> profile does not support complex types when you specify vectorized query execution (<code>VECTORIZE=true</code>).</p>
<h2 id="sample-data-set"><a class="header" href="#sample-data-set"><a id="hive_sampledset"></a>Sample Data Set</a></h2>
<p>Examples presented in this topic operate on a common data set. This simple data set models a retail sales operation and includes fields with the following names and data types:</p>
<div class="table-wrapper"><table><thead><tr><th>Column Name</th><th>Data Type</th></tr></thead><tbody>
<tr><td>location</td><td>text</td></tr>
<tr><td>month</td><td>text</td></tr>
<tr><td>number_of_orders</td><td>integer</td></tr>
<tr><td>total_sales</td><td>double</td></tr>
</tbody></table>
</div>
<p>Prepare the sample data set for use:</p>
<ol>
<li>
<p>First, create a text file:</p>
<pre><code>$ vi /tmp/pxf_hive_datafile.txt
</code></pre>
</li>
<li>
<p>Add the following data to <code>pxf_hive_datafile.txt</code>; notice the use of the comma (<code>,</code>) to separate the four field values:</p>
<pre><code>Prague,Jan,101,4875.33
Rome,Mar,87,1557.39
Bangalore,May,317,8936.99
Beijing,Jul,411,11600.67
San Francisco,Sept,156,6846.34
Paris,Nov,159,7134.56
San Francisco,Jan,113,5397.89
Prague,Dec,333,9894.77
Bangalore,Jul,271,8320.55
Beijing,Dec,100,4248.41
</code></pre>
</li>
</ol>
<p>Make note of the path to <code>pxf_hive_datafile.txt</code>; you will use it in later exercises.</p>
<h2 id="hive-command-line"><a class="header" href="#hive-command-line"><a id="hive_cmdline"></a>Hive Command Line</a></h2>
<p>The Hive command line is a subsystem similar to that of <code>psql</code>. To start the Hive command line:</p>
<pre><code class="language-shell">$ HADOOP_USER_NAME=hdfs hive
</code></pre>
<p>The default Hive database is named <code>default</code>.</p>
<h3 id="example-creating-a-hive-table"><a class="header" href="#example-creating-a-hive-table"><a id="hive_cmdline_example" class="no-quick-link"></a>Example: Creating a Hive Table</a></h3>
<p>Create a Hive table to expose the sample data set.</p>
<ol>
<li>
<p>Create a Hive table named <code>sales_info</code> in the <code>default</code> database:</p>
<pre><code class="language-sql">hive&gt; CREATE TABLE sales_info (location string, month string,
        number_of_orders int, total_sales double)
        ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
        STORED AS textfile;
</code></pre>
<p>Notice that:</p>
<ul>
<li>The <code>STORED AS textfile</code> subclause instructs Hive to create the table in Textfile (the default) format.  Hive Textfile format supports comma-, tab-, and space-separated values, as well as data specified in JSON notation.</li>
<li>The <code>DELIMITED FIELDS TERMINATED BY</code> subclause identifies the field delimiter within a data record (line). The <code>sales_info</code> table field delimiter is a comma (<code>,</code>).</li>
</ul>
</li>
<li>
<p>Load the <code>pxf_hive_datafile.txt</code> sample data file into the <code>sales_info</code> table that you just created:</p>
<pre><code class="language-sql">hive&gt; LOAD DATA LOCAL INPATH '/tmp/pxf_hive_datafile.txt'
        INTO TABLE sales_info;
</code></pre>
<p>In examples later in this section, you will access the <code>sales_info</code> Hive table directly via PXF. You will also insert <code>sales_info</code> data into tables of other Hive file format types, and use PXF to access those directly as well.</p>
</li>
<li>
<p>Perform a query on <code>sales_info</code> to verify that you loaded the data successfully:</p>
<pre><code class="language-sql">hive&gt; SELECT * FROM sales_info;
</code></pre>
</li>
</ol>
<h3 id="determining-the-hdfs-location-of-a-hive-table"><a class="header" href="#determining-the-hdfs-location-of-a-hive-table"><a id="hive_cmdline_fileloc" class="no-quick-link"></a>Determining the HDFS Location of a Hive Table</a></h3>
<p>Should you need to identify the HDFS file location of a Hive managed table, reference it using its HDFS file path. You can determine a Hive table’s location in HDFS using the <code>DESCRIBE</code> command. For example:</p>
<pre><code class="language-sql">hive&gt; DESCRIBE EXTENDED sales_info;
Detailed Table Information
...
location:hdfs://&lt;namenode&gt;:&lt;port&gt;/apps/hive/warehouse/sales_info
...
</code></pre>
<h2 id="querying-external-hive-data"><a class="header" href="#querying-external-hive-data"><a id="hive_queryextdata"></a>Querying External Hive Data</a></h2>
<p>You can create a SynxDB external table to access Hive table data.   As described previously, the PXF Hive connector defines specific profiles to support different file formats. These profiles are named <code>hive</code>, <code>hive:text</code>, <code>hive:rc</code>, and <code>hive:orc</code>.</p>
<p>The <code>hive:text</code> and <code>hive:rc</code> profiles are specifically optimized for text and RCFile formats, respectively. The <code>hive:orc</code> profile is optimized for ORC file formats. The <code>hive</code> profile is optimized for all file storage types; you can use the <code>hive</code> profile when the underlying Hive table is composed of multiple partitions with differing file formats.</p>
<p>PXF uses column projection to increase query performance when you access a Hive table using the <code>hive</code>, <code>hive:rc</code>, or <code>hive:orc</code> profiles.</p>
<p>Use the following syntax to create a SynxDB external table that references a Hive table:</p>
<pre><code class="language-sql">CREATE EXTERNAL TABLE &lt;table_name&gt;
    ( &lt;column_name&gt; &lt;data_type&gt; [, ...] | LIKE &lt;other_table&gt; )
LOCATION ('pxf://&lt;hive-db-name&gt;.&lt;hive-table-name&gt;
    ?PROFILE=&lt;profile_name&gt;[&amp;SERVER=&lt;server_name&gt;][&amp;PPD=&lt;boolean&gt;][&amp;VECTORIZE=&lt;boolean&gt;]')
FORMAT 'CUSTOM|TEXT' (FORMATTER='pxfwritable_import' | delimiter='&lt;delim&gt;')
</code></pre>
<p>Hive connector-specific keywords and values used in the SynxDB <a href="../ref_guide/sql_commands/CREATE_EXTERNAL_TABLE.html">CREATE EXTERNAL TABLE</a> call are described below.</p>
<div class="table-wrapper"><table><thead><tr><th>Keyword</th><th>Value</th></tr></thead><tbody>
<tr><td>&lt;hive‑db‑name&gt;</td><td>The name of the Hive database. If omitted, defaults to the Hive database named <code>default</code>.</td></tr>
<tr><td>&lt;hive‑table‑name&gt;</td><td>The name of the Hive table.</td></tr>
<tr><td>PROFILE=&lt;profile_name&gt;</td><td><code>&lt;profile_name&gt;</code> must specify one of the values <code>hive</code>, <code>hive:text</code>, <code>hive:rc</code>, or <code>hive:orc</code>.</td></tr>
<tr><td>SERVER=&lt;server_name&gt;</td><td>The named server configuration that PXF uses to access the data. PXF uses the <code>default</code> server if not specified.</td></tr>
<tr><td>PPD=&lt;boolean&gt;</td><td>Activate or deactivate predicate pushdown for all queries on this table; this option applies only to the <code>hive</code>, <code>hive:orc</code>, and <code>hive:rc</code> profiles, and overrides a <code>pxf.ppd.hive</code> property setting in the &lt;server_name&gt; configuration.</td></tr>
<tr><td>VECTORIZE=&lt;boolean&gt;</td><td>When <code>PROFILE=hive:orc</code>, a Boolean value that specifies whether or not PXF uses vectorized query execution when accessing the underlying ORC files. The default value is <code>false</code>, does not use vectorized query execution.</td></tr>
<tr><td>FORMAT (<code>hive</code> and <code>hive:orc</code> profiles)</td><td>The <code>FORMAT</code> clause must specify <code>'CUSTOM'</code>. The <code>CUSTOM</code> format requires the built-in <code>pxfwritable_import</code> <code>formatter</code>.</td></tr>
<tr><td>FORMAT (<code>hive:text</code> and <code>hive:rc</code> profiles)</td><td>The <code>FORMAT</code> clause must specify <code>TEXT</code>. Specify the single ascii character field delimiter in the <code>delimiter='&lt;delim&gt;'</code> formatting option.</td></tr>
</tbody></table>
</div><div class="note"><b>Note:</b> Because Hive tables can be backed by one or more files and each file can have a unique layout or schema, PXF requires that the column names that you specify when you create the external table match the column names defined for the Hive table. This allows you to:<ul>
<li>Create the PXF external table with columns in a different order than the Hive table.</li>
<li>Create a PXF external table that reads a subset of the columns in the Hive table.</li>
<li>Read a Hive table where the files backing the table have a different number of columns.</li></ul></div>
<h2 id="accessing-textfile-format-hive-tables"><a class="header" href="#accessing-textfile-format-hive-tables"><a id="hive_text"></a>Accessing TextFile-Format Hive Tables</a></h2>
<p>You can use the <code>hive</code> and <code>hive:text</code> profiles to access Hive table data stored in TextFile format.</p>
<h3 id="example-using-the-hive-profile"><a class="header" href="#example-using-the-hive-profile"><a id="hive_hive_example" class="no-quick-link"></a>Example: Using the hive Profile</a></h3>
<p>Use the <code>hive</code> profile to create a readable SynxDB external table that references the Hive <code>sales_info</code> textfile format table that you created earlier.</p>
<ol>
<li>
<p>Create the external table:</p>
<pre><code class="language-sql">postgres=# CREATE EXTERNAL TABLE salesinfo_hiveprofile(location text, month text, number_of_orders int, total_sales float8)
            LOCATION ('pxf://default.sales_info?PROFILE=hive')
          FORMAT 'custom' (FORMATTER='pxfwritable_import');
</code></pre>
</li>
<li>
<p>Query the table:</p>
<pre><code class="language-sql">postgres=# SELECT * FROM salesinfo_hiveprofile;
</code></pre>
<pre><code class="language-shell">   location    | month | number_of_orders | total_sales
---------------+-------+------------------+-------------
 Prague        | Jan   |              101 |     4875.33
 Rome          | Mar   |               87 |     1557.39
 Bangalore     | May   |              317 |     8936.99
 ...
</code></pre>
</li>
</ol>
<h3 id="example-using-the-hivetext-profile"><a class="header" href="#example-using-the-hivetext-profile"><a id="hive_hivetext_example" class="no-quick-link"></a>Example: Using the hive:text Profile</a></h3>
<p>Use the PXF <code>hive:text</code> profile to create a readable SynxDB external table from the Hive <code>sales_info</code> textfile format table that you created earlier.</p>
<ol>
<li>
<p>Create the external table:</p>
<pre><code class="language-sql">postgres=# CREATE EXTERNAL TABLE salesinfo_hivetextprofile(location text, month text, number_of_orders int, total_sales float8)
             LOCATION ('pxf://default.sales_info?PROFILE=hive:text')
           FORMAT 'TEXT' (delimiter=E',');
</code></pre>
<p>Notice that the <code>FORMAT</code> subclause <code>delimiter</code> value is specified as the single ascii comma character <code>','</code>. <code>E</code> escapes the character.</p>
</li>
<li>
<p>Query the external table:</p>
<pre><code class="language-sql">postgres=# SELECT * FROM salesinfo_hivetextprofile WHERE location='Beijing';
</code></pre>
<pre><code class="language-shell"> location | month | number_of_orders | total_sales
----------+-------+------------------+-------------
 Beijing  | Jul   |              411 |    11600.67
 Beijing  | Dec   |              100 |     4248.41
(2 rows)
</code></pre>
</li>
</ol>
<h2 id="accessing-rcfile-format-hive-tables"><a class="header" href="#accessing-rcfile-format-hive-tables"><a id="hive_hiverc"></a>Accessing RCFile-Format Hive Tables</a></h2>
<p>The RCFile Hive table format is used for row columnar formatted data. The PXF <code>hive:rc</code> profile provides access to RCFile data.</p>
<h3 id="example-using-the-hiverc-profile"><a class="header" href="#example-using-the-hiverc-profile"><a id="hive_hiverc_example" class="no-quick-link"></a>Example: Using the hive:rc Profile</a></h3>
<p>Use the <code>hive:rc</code> profile to query RCFile-formatted data in a Hive table.</p>
<ol>
<li>
<p>Start the <code>hive</code> command line and create a Hive table stored in RCFile format:</p>
<pre><code class="language-shell">$ HADOOP_USER_NAME=hdfs hive
</code></pre>
<pre><code class="language-sql">hive&gt; CREATE TABLE sales_info_rcfile (location string, month string,
        number_of_orders int, total_sales double)
      ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
      STORED AS rcfile;
</code></pre>
</li>
<li>
<p>Insert the data from the <code>sales_info</code> table into <code>sales_info_rcfile</code>:</p>
<pre><code class="language-sql">hive&gt; INSERT INTO TABLE sales_info_rcfile SELECT * FROM sales_info;
</code></pre>
<p>A copy of the sample data set is now stored in RCFile format in the Hive <code>sales_info_rcfile</code> table.</p>
</li>
<li>
<p>Query the <code>sales_info_rcfile</code> Hive table to verify that the data was loaded correctly:</p>
<pre><code class="language-sql">hive&gt; SELECT * FROM sales_info_rcfile;
</code></pre>
</li>
<li>
<p>Use the PXF <code>hive:rc</code> profile to create a readable SynxDB external table that references the Hive <code>sales_info_rcfile</code> table that you created in the previous steps. For example:</p>
<pre><code class="language-sql">postgres=# CREATE EXTERNAL TABLE salesinfo_hivercprofile(location text, month text, number_of_orders int, total_sales float8)
             LOCATION ('pxf://default.sales_info_rcfile?PROFILE=hive:rc')
           FORMAT 'TEXT' (delimiter=E',');
</code></pre>
</li>
<li>
<p>Query the external table:</p>
<pre><code class="language-sql">postgres=# SELECT location, total_sales FROM salesinfo_hivercprofile;
</code></pre>
<pre><code class="language-shell">   location    | total_sales
---------------+-------------
 Prague        |     4875.33
 Rome          |     1557.39
 Bangalore     |     8936.99
 Beijing       |    11600.67
 ...
</code></pre>
</li>
</ol>
<h2 id="accessing-orc-format-hive-tables"><a class="header" href="#accessing-orc-format-hive-tables"><a id="hive_orc"></a>Accessing ORC-Format Hive Tables</a></h2>
<p>The Optimized Row Columnar (ORC) file format is a columnar file format that provides a highly efficient way to both store and access HDFS data. ORC format offers improvements over text and RCFile formats in terms of both compression and performance. PXF supports ORC version 1.2.1.</p>
<p>ORC is type-aware and specifically designed for Hadoop workloads. ORC files store both the type of and encoding information for the data in the file. All columns within a single group of row data (also known as stripe) are stored together on disk in ORC format files. The columnar nature of the ORC format type enables read projection, helping avoid accessing unnecessary columns during a query.</p>
<p>ORC also supports predicate pushdown with built-in indexes at the file, stripe, and row levels, moving the filter operation to the data loading phase.</p>
<p>Refer to the <a href="https://orc.apache.org/docs/">Apache orc</a> and the Apache Hive <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC">LanguageManual ORC</a> websites for detailed information about the ORC file format.</p>
<h3 id="profiles-supporting-the-orc-file-format"><a class="header" href="#profiles-supporting-the-orc-file-format"><a id="orc-profiles" class="no-quick-link"></a>Profiles Supporting the ORC File Format</a></h3>
<p>When choosing an ORC-supporting profile, consider the following:</p>
<ul>
<li>
<p>The <code>hive:orc</code> profile:</p>
<ul>
<li>Reads a single row of data at a time.</li>
<li>Supports column projection.</li>
<li>Supports complex types. You can access Hive tables composed of array, map, struct, and union data types. PXF serializes each of these complex types to <code>text</code>.</li>
</ul>
</li>
<li>
<p>The <code>hive:orc</code> profile with <code>VECTORIZE=true</code>:</p>
<ul>
<li>Reads up to 1024 rows of data at once.</li>
<li>Supports column projection.</li>
<li>Does not support complex types or the timestamp data type.</li>
</ul>
</li>
</ul>
<h3 id="example-using-the-hiveorc-profile"><a class="header" href="#example-using-the-hiveorc-profile"><a id="hive_hiveorc_example" class="no-quick-link"></a>Example: Using the hive:orc Profile</a></h3>
<p>In the following example, you will create a Hive table stored in ORC format and use the <code>hive:orc</code> profile to query this Hive table.</p>
<ol>
<li>
<p>Create a Hive table with ORC file format:</p>
<pre><code class="language-shell">$ HADOOP_USER_NAME=hdfs hive
</code></pre>
<pre><code class="language-sql">hive&gt; CREATE TABLE sales_info_ORC (location string, month string,
        number_of_orders int, total_sales double)
      STORED AS ORC;
</code></pre>
</li>
<li>
<p>Insert the data from the <code>sales_info</code> table into <code>sales_info_ORC</code>:</p>
<pre><code class="language-sql">hive&gt; INSERT INTO TABLE sales_info_ORC SELECT * FROM sales_info;
</code></pre>
<p>A copy of the sample data set is now stored in ORC format in <code>sales_info_ORC</code>.</p>
</li>
<li>
<p>Perform a Hive query on <code>sales_info_ORC</code> to verify that the data was loaded successfully:</p>
<pre><code class="language-sql">hive&gt; SELECT * FROM sales_info_ORC;
</code></pre>
</li>
<li>
<p>Start the <code>psql</code> subsystem and turn on timing:</p>
<pre><code class="language-shell">$ psql -d postgres
</code></pre>
<pre><code class="language-sql">postgres=&gt; \timing
Timing is on.
</code></pre>
</li>
<li>
<p>Use the PXF <code>hive:orc</code> profile to create a SynxDB external table that references the Hive table named <code>sales_info_ORC</code> you created in Step 1. The <code>FORMAT</code> clause must specify <code>'CUSTOM'</code>. The <code>hive:orc</code> <code>CUSTOM</code> format supports only the built-in <code>'pxfwritable_import'</code> <code>formatter</code>.</p>
<pre><code class="language-sql">postgres=&gt; CREATE EXTERNAL TABLE salesinfo_hiveORCprofile(location text, month text, number_of_orders int, total_sales float8)
             LOCATION ('pxf://default.sales_info_ORC?PROFILE=hive:orc')
             FORMAT 'CUSTOM' (FORMATTER='pxfwritable_import');
</code></pre>
</li>
<li>
<p>Query the external table:</p>
<pre><code class="language-sql">postgres=&gt; SELECT * FROM salesinfo_hiveORCprofile;
</code></pre>
<pre><code class="language-pre">   location    | month | number_of_orders | total_sales 
---------------+-------+------------------+-------------
 Prague        | Jan   |              101 |     4875.33
 Rome          | Mar   |               87 |     1557.39
 Bangalore     | May   |              317 |     8936.99
 ...

Time: 425.416 ms
</code></pre>
</li>
</ol>
<h3 id="example-using-the-vectorized-hiveorc-profile"><a class="header" href="#example-using-the-vectorized-hiveorc-profile"><a id="hive_hivevectorizedorc_example" class="no-quick-link"></a>Example: Using the Vectorized hive:orc Profile</a></h3>
<p>In the following example, you will use the vectorized <code>hive:orc</code> profile to query the <code>sales_info_ORC</code> Hive table that you created in the previous example.</p>
<ol>
<li>
<p>Start the <code>psql</code> subsystem:</p>
<pre><code class="language-shell">$ psql -d postgres
</code></pre>
</li>
<li>
<p>Use the PXF <code>hive:orc</code> profile to create a readable SynxDB external table that references the Hive table named <code>sales_info_ORC</code> that you created in Step 1 of the previous example. The <code>FORMAT</code> clause must specify <code>'CUSTOM'</code>. The <code>hive:orc</code> <code>CUSTOM</code> format supports only the built-in <code>'pxfwritable_import'</code> <code>formatter</code>.</p>
<pre><code class="language-sql">postgres=&gt; CREATE EXTERNAL TABLE salesinfo_hiveVectORC(location text, month text, number_of_orders int, total_sales float8)
             LOCATION ('pxf://default.sales_info_ORC?PROFILE=hive:orc&amp;VECTORIZE=true')
             FORMAT 'CUSTOM' (FORMATTER='pxfwritable_import');
</code></pre>
</li>
<li>
<p>Query the external table:</p>
<pre><code class="language-sql">postgres=&gt; SELECT * FROM salesinfo_hiveVectORC;
</code></pre>
<pre><code class="language-pre">   location    | month | number_of_orders | total_sales 
---------------+-------+------------------+-------------
 Prague        | Jan   |              101 |     4875.33
 Rome          | Mar   |               87 |     1557.39
 Bangalore     | May   |              317 |     8936.99
 ...

Time: 425.416 ms
</code></pre>
</li>
</ol>
<h2 id="accessing-parquet-format-hive-tables"><a class="header" href="#accessing-parquet-format-hive-tables"><a id="hive_parquet"></a>Accessing Parquet-Format Hive Tables</a></h2>
<p>The PXF <code>hive</code> profile supports both non-partitioned and partitioned Hive tables that use the Parquet storage format. Map the table columns using equivalent SynxDB data types. For example, if a Hive table is created in the <code>default</code> schema using:</p>
<pre><code class="language-sql">hive&gt; CREATE TABLE hive_parquet_table (location string, month string,
            number_of_orders int, total_sales double)
        STORED AS parquet;
</code></pre>
<p>Define the SynxDB external table:</p>
<pre><code class="language-sql">postgres=# CREATE EXTERNAL TABLE pxf_parquet_table (location text, month text, number_of_orders int, total_sales double precision)
    LOCATION ('pxf://default.hive_parquet_table?profile=hive')
    FORMAT 'CUSTOM' (FORMATTER='pxfwritable_import');
</code></pre>
<p>And query the table:</p>
<pre><code class="language-sql">postgres=# SELECT month, number_of_orders FROM pxf_parquet_table;
</code></pre>
<h2 id="accessing-avro-format-hive-tables"><a class="header" href="#accessing-avro-format-hive-tables"><a id="hive_avro"></a>Accessing Avro-Format Hive Tables</a></h2>
<p>The PXF <code>hive</code> profile supports accessing Hive tables that use the Avro storage format. Map the table columns using equivalent SynxDB data types. For example, if a Hive table is created in the <code>default</code> schema using:</p>
<pre><code class="language-sql">hive&gt; CREATE TABLE hive_avro_data_table (id int, name string, user_id string)
	ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
	STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
	OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat';
</code></pre>
<p>Define the SynxDB external table:</p>
<pre><code class="language-sql">postgres=# CREATE EXTERNAL TABLE userinfo_hiveavro(id int, name text, user_id text)
	LOCATION ('pxf://default.hive_avro_data_table?profile=hive')
	FORMAT 'custom' (FORMATTER='pxfwritable_import');
</code></pre>
<p>And query the table:</p>
<pre><code class="language-sql">postgres=# SELECT * FROM userinfo_hiveavro;
</code></pre>
<h2 id="working-with-complex-data-types"><a class="header" href="#working-with-complex-data-types"><a id="hive_complex"></a>Working with Complex Data Types</a></h2>
<h3 id="example-using-the-hive-profile-with-complex-data-types"><a class="header" href="#example-using-the-hive-profile-with-complex-data-types"><a id="hive_complex_hive" class="no-quick-link"></a>Example: Using the hive Profile with Complex Data Types</a></h3>
<p>This example employs the <code>hive</code> profile and the array and map complex types, specifically an array of integers and a string key/value pair map.</p>
<p>The data schema for this example includes fields with the following names and data types:</p>
<div class="table-wrapper"><table><thead><tr><th>Column Name</th><th>Data Type</th></tr></thead><tbody>
<tr><td>index</td><td>int</td></tr>
<tr><td>name</td><td>string</td></tr>
<tr><td>intarray</td><td>array of integers</td></tr>
<tr><td>propmap</td><td>map of string key and value pairs</td></tr>
</tbody></table>
</div>
<p>When you specify an array field in a Hive table, you must identify the terminator for each item in the collection. Similarly, you must also specify the map key termination character.</p>
<ol>
<li>
<p>Create a text file from which you will load the data set:</p>
<pre><code>$ vi /tmp/pxf_hive_complex.txt
</code></pre>
</li>
<li>
<p>Add the following text to <code>pxf_hive_complex.txt</code>.  This data uses a comma (<code>,</code>) to separate field values, the percent symbol <code>%</code> to separate collection items, and a <code>:</code> to terminate map key values:</p>
<pre><code>3,Prague,1%2%3,zone:euro%status:up
89,Rome,4%5%6,zone:euro
400,Bangalore,7%8%9,zone:apac%status:pending
183,Beijing,0%1%2,zone:apac
94,Sacramento,3%4%5,zone:noam%status:down
101,Paris,6%7%8,zone:euro%status:up
56,Frankfurt,9%0%1,zone:euro
202,Jakarta,2%3%4,zone:apac%status:up
313,Sydney,5%6%7,zone:apac%status:pending
76,Atlanta,8%9%0,zone:noam%status:down
</code></pre>
</li>
<li>
<p>Create a Hive table to represent this data:</p>
<pre><code class="language-shell">$ HADOOP_USER_NAME=hdfs hive
</code></pre>
<pre><code class="language-sql">hive&gt; CREATE TABLE table_complextypes( index int, name string, intarray ARRAY&lt;int&gt;, propmap MAP&lt;string, string&gt;)
         ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
         COLLECTION ITEMS TERMINATED BY '%'
         MAP KEYS TERMINATED BY ':'
         STORED AS TEXTFILE;
</code></pre>
<p>Notice that:</p>
<ul>
<li><code>FIELDS TERMINATED BY</code> identifies a comma as the field terminator.</li>
<li>The <code>COLLECTION ITEMS TERMINATED BY</code> subclause specifies the percent sign as the collection items (array item, map key/value pair) terminator.</li>
<li><code>MAP KEYS TERMINATED BY</code> identifies a colon as the terminator for map keys.</li>
</ul>
</li>
<li>
<p>Load the <code>pxf_hive_complex.txt</code> sample data file into the <code>table_complextypes</code> table that you just created:</p>
<pre><code class="language-sql">hive&gt; LOAD DATA LOCAL INPATH '/tmp/pxf_hive_complex.txt' INTO TABLE table_complextypes;
</code></pre>
</li>
<li>
<p>Perform a query on Hive table <code>table_complextypes</code> to verify that the data was loaded successfully:</p>
<pre><code class="language-sql">hive&gt; SELECT * FROM table_complextypes;
</code></pre>
<pre><code class="language-shell">3	Prague	[1,2,3]	{"zone":"euro","status":"up"}
89	Rome	[4,5,6]	{"zone":"euro"}
400	Bangalore	[7,8,9]	{"zone":"apac","status":"pending"}
...
</code></pre>
</li>
<li>
<p>Use the PXF <code>hive</code> profile to create a readable SynxDB external table that references the Hive table named <code>table_complextypes</code>:</p>
<pre><code class="language-sql">postgres=# CREATE EXTERNAL TABLE complextypes_hiveprofile(index int, name text, intarray text, propmap text)
             LOCATION ('pxf://table_complextypes?PROFILE=hive')
           FORMAT 'CUSTOM' (FORMATTER='pxfwritable_import');
</code></pre>
<p>Notice that the integer array and map complex types are mapped to SynxDB data type text.</p>
</li>
<li>
<p>Query the external table:</p>
<pre><code class="language-sql">postgres=# SELECT * FROM complextypes_hiveprofile;
</code></pre>
<pre><code class="language-shell"> index |    name    | intarray |              propmap
-------+------------+----------+------------------------------------
     3 | Prague     | [1,2,3]  | {"zone":"euro","status":"up"}
    89 | Rome       | [4,5,6]  | {"zone":"euro"}
   400 | Bangalore  | [7,8,9]  | {"zone":"apac","status":"pending"}
   183 | Beijing    | [0,1,2]  | {"zone":"apac"}
    94 | Sacramento | [3,4,5]  | {"zone":"noam","status":"down"}
   101 | Paris      | [6,7,8]  | {"zone":"euro","status":"up"}
    56 | Frankfurt  | [9,0,1]  | {"zone":"euro"}
   202 | Jakarta    | [2,3,4]  | {"zone":"apac","status":"up"}
   313 | Sydney     | [5,6,7]  | {"zone":"apac","status":"pending"}
    76 | Atlanta    | [8,9,0]  | {"zone":"noam","status":"down"}
(10 rows)
</code></pre>
<p><code>intarray</code> and <code>propmap</code> are each serialized as text strings.</p>
</li>
</ol>
<h3 id="example-using-the-hiveorc-profile-with-complex-data-types"><a class="header" href="#example-using-the-hiveorc-profile-with-complex-data-types"><a id="hive_complex_hiveorc" class="no-quick-link"></a>Example: Using the hive:orc Profile with Complex Data Types</a></h3>
<p>In the following example, you will create and populate a Hive table stored in ORC format. You will use the <code>hive:orc</code> profile to query the complex types in this Hive table.</p>
<ol>
<li>
<p>Create a Hive table with ORC storage format:</p>
<pre><code class="language-shell">$ HADOOP_USER_NAME=hdfs hive
</code></pre>
<pre><code class="language-sql">hive&gt; CREATE TABLE table_complextypes_ORC( index int, name string, intarray ARRAY&lt;int&gt;, propmap MAP&lt;string, string&gt;)
        ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
        COLLECTION ITEMS TERMINATED BY '%'
        MAP KEYS TERMINATED BY ':'
      STORED AS ORC;
</code></pre>
</li>
<li>
<p>Insert the data from the <code>table_complextypes</code> table that you created in the previous example into <code>table_complextypes_ORC</code>:</p>
<pre><code class="language-sql">hive&gt; INSERT INTO TABLE table_complextypes_ORC SELECT * FROM table_complextypes;
</code></pre>
<p>A copy of the sample data set is now stored in ORC format in <code>table_complextypes_ORC</code>.</p>
</li>
<li>
<p>Perform a Hive query on <code>table_complextypes_ORC</code> to verify that the data was loaded successfully:</p>
<pre><code class="language-sql">hive&gt; SELECT * FROM table_complextypes_ORC;
</code></pre>
<pre><code class="language-pre">OK
3       Prague       [1,2,3]    {"zone":"euro","status":"up"}
89      Rome         [4,5,6]    {"zone":"euro"}
400     Bangalore    [7,8,9]    {"zone":"apac","status":"pending"}
...
</code></pre>
</li>
<li>
<p>Start the <code>psql</code> subsystem:</p>
<pre><code class="language-shell">$ psql -d postgres
</code></pre>
</li>
<li>
<p>Use the PXF <code>hive:orc</code> profile to create a readable SynxDB external table from the Hive table named <code>table_complextypes_ORC</code> you created in Step 1. The <code>FORMAT</code> clause must specify <code>'CUSTOM'</code>. The <code>hive:orc</code> <code>CUSTOM</code> format supports only the built-in <code>'pxfwritable_import'</code> <code>formatter</code>.</p>
<pre><code class="language-sql">postgres=&gt; CREATE EXTERNAL TABLE complextypes_hiveorc(index int, name text, intarray text, propmap text)
           LOCATION ('pxf://default.table_complextypes_ORC?PROFILE=hive:orc')
             FORMAT 'CUSTOM' (FORMATTER='pxfwritable_import');
</code></pre>
<p>Notice that the integer array and map complex types are again mapped to SynxDB data type text.</p>
</li>
<li>
<p>Query the external table:</p>
<pre><code class="language-sql">postgres=&gt; SELECT * FROM complextypes_hiveorc;
</code></pre>
<pre><code class="language-pre"> index |    name    | intarray |              propmap               
-------+------------+----------+------------------------------------
     3 | Prague     | [1,2,3]  | {"zone":"euro","status":"up"}
    89 | Rome       | [4,5,6]  | {"zone":"euro"}
   400 | Bangalore  | [7,8,9]  | {"zone":"apac","status":"pending"}
 ...

</code></pre>
<p><code>intarray</code> and <code>propmap</code> are again serialized as text strings.</p>
</li>
</ol>
<h2 id="partition-pruning"><a class="header" href="#partition-pruning"><a id="partitionfiltering"></a>Partition Pruning</a></h2>
<p>The PXF Hive connector supports Hive partition pruning and the Hive partition directory structure. This enables partition exclusion on selected HDFS files comprising a Hive table. To use the partition filtering feature to reduce network traffic and I/O, run a query on a PXF external table using a <code>WHERE</code> clause that refers to a specific partition column in a partitioned Hive table.</p>
<p>The PXF Hive Connector partition filtering support for Hive string and integral types is described below:</p>
<ul>
<li>The relational operators <code>=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code>, and <code>&lt;&gt;</code> are supported on string types.</li>
<li>The relational operators <code>=</code> and <code>&lt;&gt;</code> are supported on integral types (To use partition filtering with Hive integral types, you must update the Hive configuration as described in the <a href="#prereq">Prerequisites</a>).</li>
<li>The logical operators <code>AND</code> and <code>OR</code> are supported when used with the relational operators mentioned above.</li>
<li>The <code>LIKE</code> string operator is not supported.</li>
</ul>
<p>To take advantage of PXF partition filtering pushdown, the Hive and PXF partition field names must be the same. Otherwise, PXF ignores partition filtering and the filtering is performed on the SynxDB side, impacting performance.</p>
<div class="note"><b>Note:</b> The PXF Hive connector filters only on partition columns, not on other table attributes. Additionally, filter pushdown is supported only for those data types and operators identified above.</div>
<p>PXF filter pushdown is enabled by default. You configure PXF filter pushdown as described in <a href="filter_push.html">About Filter Pushdown</a>.</p>
<h3 id="example-using-the-hive-profile-to-access-partitioned-homogenous-data"><a class="header" href="#example-using-the-hive-profile-to-access-partitioned-homogenous-data"><a id="hive_homog_part"></a>Example: Using the hive Profile to Access Partitioned Homogenous Data</a></h3>
<p>In this example, you use the <code>hive</code> profile to query a Hive table named <code>sales_part</code> that you partition on the <code>delivery_state</code> and <code>delivery_city</code> fields. You then create a SynxDB external table to query <code>sales_part</code>. The procedure includes specific examples that illustrate filter pushdown.</p>
<ol>
<li>
<p>Create a Hive table named <code>sales_part</code> with two partition columns, <code>delivery_state</code> and <code>delivery_city:</code></p>
<pre><code class="language-sql">hive&gt; CREATE TABLE sales_part (cname string, itype string, supplier_key int, price double)
        PARTITIONED BY (delivery_state string, delivery_city string)
        ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
</code></pre>
</li>
<li>
<p>Load data into this Hive table and add some partitions:</p>
<pre><code class="language-sql">hive&gt; INSERT INTO TABLE sales_part
        PARTITION(delivery_state = 'CALIFORNIA', delivery_city = 'Fresno')
        VALUES ('block', 'widget', 33, 15.17);
hive&gt; INSERT INTO TABLE sales_part
        PARTITION(delivery_state = 'CALIFORNIA', delivery_city = 'Sacramento')
        VALUES ('cube', 'widget', 11, 1.17);
hive&gt; INSERT INTO TABLE sales_part
        PARTITION(delivery_state = 'NEVADA', delivery_city = 'Reno')
        VALUES ('dowel', 'widget', 51, 31.82);
hive&gt; INSERT INTO TABLE sales_part
        PARTITION(delivery_state = 'NEVADA', delivery_city = 'Las Vegas')
        VALUES ('px49', 'pipe', 52, 99.82);
</code></pre>
</li>
<li>
<p>Query the <code>sales_part</code> table:</p>
<pre><code class="language-sql">hive&gt; SELECT * FROM sales_part;
</code></pre>
<p>A <code>SELECT *</code> statement on a Hive partitioned table shows the partition fields at the end of the record.</p>
</li>
<li>
<p>Examine the Hive/HDFS directory structure for the <code>sales_part</code> table:</p>
<pre><code class="language-shell">$ sudo -u hdfs hdfs dfs -ls -R /apps/hive/warehouse/sales_part
/apps/hive/warehouse/sales_part/delivery_state=CALIFORNIA/delivery_city=Fresno/
/apps/hive/warehouse/sales_part/delivery_state=CALIFORNIA/delivery_city=Sacramento/
/apps/hive/warehouse/sales_part/delivery_state=NEVADA/delivery_city=Reno/
/apps/hive/warehouse/sales_part/delivery_state=NEVADA/delivery_city=Las Vegas/
</code></pre>
</li>
<li>
<p>Create a PXF external table to read the partitioned <code>sales_part</code> Hive table.  To take advantage of partition filter push-down, define fields corresponding to the Hive partition fields at the end of the <code>CREATE EXTERNAL TABLE</code> attribute list.</p>
<pre><code class="language-shell">$ psql -d postgres
</code></pre>
<pre><code class="language-sql">postgres=# CREATE EXTERNAL TABLE pxf_sales_part(
             cname TEXT, itype TEXT,
             supplier_key INTEGER, price DOUBLE PRECISION,
             delivery_state TEXT, delivery_city TEXT)
           LOCATION ('pxf://sales_part?PROFILE=hive')
           FORMAT 'CUSTOM' (FORMATTER='pxfwritable_import');
</code></pre>
</li>
<li>
<p>Query the table:</p>
<pre><code class="language-sql">postgres=# SELECT * FROM pxf_sales_part;
</code></pre>
</li>
<li>
<p>Perform another query (no pushdown) on <code>pxf_sales_part</code> to return records where the <code>delivery_city</code> is <code>Sacramento</code> and  <code>cname</code> is <code>cube</code>:</p>
<pre><code class="language-sql">postgres=# SELECT * FROM pxf_sales_part WHERE delivery_city = 'Sacramento' AND cname = 'cube';
</code></pre>
<p>The query filters the <code>delivery_city</code> partition <code>Sacramento</code>. The filter on <code>cname</code> is not pushed down, since it is not a partition column. It is performed on the SynxDB side after all the data in the <code>Sacramento</code> partition is transferred for processing.</p>
</li>
<li>
<p>Query (with pushdown) for all records where <code>delivery_state</code> is <code>CALIFORNIA</code>:</p>
<pre><code class="language-sql">postgres=# SET gp_external_enable_filter_pushdown=on;
postgres=# SELECT * FROM pxf_sales_part WHERE delivery_state = 'CALIFORNIA';
</code></pre>
<p>This query reads all of the data in the <code>CALIFORNIA</code> <code>delivery_state</code> partition, regardless of the city.</p>
</li>
</ol>
<h3 id="example-using-the-hive-profile-to-access-partitioned-heterogeneous-data"><a class="header" href="#example-using-the-hive-profile-to-access-partitioned-heterogeneous-data"><a id="hive_heter_part"></a>Example: Using the hive Profile to Access Partitioned Heterogeneous Data</a></h3>
<p>You can use the PXF <code>hive</code> profile with any Hive file storage types. With the <code>hive</code> profile, you can access heterogeneous format data in a single Hive table where the partitions may be stored in different file formats.</p>
<p>In this example, you create a partitioned Hive external table. The table is composed of the HDFS data files associated with the <code>sales_info</code> (text format) and <code>sales_info_rcfile</code> (RC format) Hive tables that you created in previous exercises. You will partition the data by year, assigning the data from <code>sales_info</code> to the year 2013, and the data from <code>sales_info_rcfile</code> to the year 2016. (Ignore at the moment the fact that the tables contain the same data.) You will then use the PXF <code>hive</code> profile to query this partitioned Hive external table.</p>
<ol>
<li>
<p>Create a Hive external table named <code>hive_multiformpart</code> that is partitioned by a string field named <code>year</code>:</p>
<pre><code class="language-shell">$ HADOOP_USER_NAME=hdfs hive
</code></pre>
<pre><code class="language-sql">hive&gt; CREATE EXTERNAL TABLE hive_multiformpart( location string, month string, number_of_orders int, total_sales double)
        PARTITIONED BY( year string )
        ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
</code></pre>
</li>
<li>
<p>Describe the <code>sales_info</code> and <code>sales_info_rcfile</code> tables, noting the HDFS file <code>location</code> for each table:</p>
<pre><code class="language-sql">hive&gt; DESCRIBE EXTENDED sales_info;
hive&gt; DESCRIBE EXTENDED sales_info_rcfile;
</code></pre>
</li>
<li>
<p>Create partitions in the <code>hive_multiformpart</code> table for the HDFS file locations associated with each of the <code>sales_info</code> and <code>sales_info_rcfile</code> tables:</p>
<pre><code class="language-sql">hive&gt; ALTER TABLE hive_multiformpart ADD PARTITION (year = '2013') LOCATION 'hdfs://namenode:8020/apps/hive/warehouse/sales_info';
hive&gt; ALTER TABLE hive_multiformpart ADD PARTITION (year = '2016') LOCATION 'hdfs://namenode:8020/apps/hive/warehouse/sales_info_rcfile';
</code></pre>
</li>
<li>
<p>Explicitly identify the file format of the partition associated with the  <code>sales_info_rcfile</code> table:</p>
<pre><code class="language-sql">hive&gt; ALTER TABLE hive_multiformpart PARTITION (year='2016') SET FILEFORMAT RCFILE;
</code></pre>
<p>You need not specify the file format of the partition associated with the <code>sales_info</code> table, as <code>TEXTFILE</code> format is the default.</p>
</li>
<li>
<p>Query the <code>hive_multiformpart</code> table:</p>
<pre><code class="language-sql">hive&gt; SELECT * from hive_multiformpart;
...
Bangalore	Jul	271	8320.55	2016
Beijing	Dec	100	4248.41	2016
Prague	Jan	101	4875.33	2013
Rome	Mar	87	1557.39	2013
...
hive&gt; SELECT * from hive_multiformpart WHERE year='2013';
hive&gt; SELECT * from hive_multiformpart WHERE year='2016';
</code></pre>
</li>
<li>
<p>Show the partitions defined for the <code>hive_multiformpart</code> table and exit <code>hive</code>:</p>
<pre><code class="language-sql">hive&gt; SHOW PARTITIONS hive_multiformpart;
year=2013
year=2016
hive&gt; quit;
</code></pre>
</li>
<li>
<p>Start the <code>psql</code> subsystem:</p>
<pre><code class="language-shell">$ psql -d postgres
</code></pre>
</li>
<li>
<p>Use the PXF <code>hive</code> profile to create a readable SynxDB external table that references the Hive <code>hive_multiformpart</code> external table that you created in the previous steps:</p>
<pre><code class="language-sql">postgres=# CREATE EXTERNAL TABLE pxf_multiformpart(location text, month text, number_of_orders int, total_sales float8, year text)
             LOCATION ('pxf://default.hive_multiformpart?PROFILE=hive')
           FORMAT 'CUSTOM' (FORMATTER='pxfwritable_import');
</code></pre>
</li>
<li>
<p>Query the PXF external table:</p>
<pre><code class="language-sql">postgres=# SELECT * FROM pxf_multiformpart;
</code></pre>
<pre><code class="language-shell">   location    | month | number_of_orders | total_sales | year 
---------------+-------+------------------+-------------+--------
 ....
 Prague        | Dec   |              333 |     9894.77 | 2013
 Bangalore     | Jul   |              271 |     8320.55 | 2013
 Beijing       | Dec   |              100 |     4248.41 | 2013
 Prague        | Jan   |              101 |     4875.33 | 2016
 Rome          | Mar   |               87 |     1557.39 | 2016
 Bangalore     | May   |              317 |     8936.99 | 2016
 ....
</code></pre>
</li>
<li>
<p>Perform a second query to calculate the total number of orders for the year 2013:</p>
<pre><code class="language-sql">postgres=# SELECT sum(number_of_orders) FROM pxf_multiformpart WHERE month='Dec' AND year='2013';
 sum 
-----
 433
</code></pre>
</li>
</ol>
<h2 id="using-pxf-with-hive-default-partitions"><a class="header" href="#using-pxf-with-hive-default-partitions"><a id="default_part"></a>Using PXF with Hive Default Partitions</a></h2>
<p>This topic describes a difference in query results between Hive and PXF queries when Hive tables use a default partition. When dynamic partitioning is enabled in Hive, a partitioned table may store data in a default partition. Hive creates a default partition when the value of a partitioning column does not match the defined type of the column (for example, when a NULL value is used for any partitioning column). In Hive, any query that includes a filter on a partition column <em>excludes</em> any data that is stored in the table’s default partition.</p>
<p>Similar to Hive, PXF represents a table’s partitioning columns as columns that are appended to the end of the table. However, PXF translates any column value in a default partition to a NULL value. This means that a SynxDB query that includes an <code>IS NULL</code> filter on a partitioning column can return different results than the same Hive query.</p>
<p>Consider a Hive partitioned table that is created with the statement:</p>
<pre><code class="language-sql">hive&gt; CREATE TABLE sales (order_id bigint, order_amount float) PARTITIONED BY (xdate date);
</code></pre>
<p>The table is loaded with five rows that contain the following data:</p>
<pre><code class="language-pre">1.0    1900-01-01
2.2    1994-04-14
3.3    2011-03-31
4.5    NULL
5.0    2013-12-06
</code></pre>
<p>Inserting row 4 creates a Hive default partition, because the partition column <code>xdate</code> contains a null value.</p>
<p>In Hive, any query that filters on the partition column omits data in the default partition. For example, the following query returns no rows:</p>
<pre><code class="language-sql">hive&gt; SELECT * FROM sales WHERE xdate IS null;
</code></pre>
<p>However, if you map this Hive table to a PXF external table in SynxDB, all default partition values are translated into actual NULL values. In SynxDB, running the same query against the PXF external table returns row 4 as the result, because the filter matches the NULL value.</p>
<p>Keep this behavior in mind when you run <code>IS NULL</code> queries on Hive partitioned tables.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../pxf/hdfs_fileasrow.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../pxf/hbase_pxf.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../pxf/hdfs_fileasrow.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../pxf/hbase_pxf.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
